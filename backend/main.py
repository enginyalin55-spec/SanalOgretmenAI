from fastapi import FastAPI, UploadFile, File, Form, HTTPException
from fastapi.middleware.cors import CORSMiddleware
from google import genai
from google.genai import types
from google.cloud import vision
from supabase import create_client, Client
from dotenv import load_dotenv
import os, json, uuid, re
import unicodedata
from pydantic import BaseModel
from typing import Union, List, Dict, Any, Optional

# =======================================================
# 1) AYARLAR VE KURULUM
# =======================================================
load_dotenv()

API_KEY = os.getenv("GEMINI_API_KEY")
if not API_KEY:
    raise RuntimeError("âŒ KRÄ°TÄ°K HATA: GEMINI_API_KEY eksik!")

SUPABASE_URL = (os.getenv("SUPABASE_URL", "") or "").rstrip("/")
SUPABASE_KEY = os.getenv("SUPABASE_KEY")
if not SUPABASE_URL or not SUPABASE_KEY:
    raise RuntimeError("âŒ KRÄ°TÄ°K HATA: SUPABASE bilgileri eksik!")

client = genai.Client(api_key=API_KEY)
supabase: Client = create_client(SUPABASE_URL, SUPABASE_KEY)

app = FastAPI(title="Sanal Ogretmen AI API", version="2.0.0 (Vision OCR + WordMask)")

app.add_middleware(
    CORSMiddleware,
    allow_origin_regex=r"https?://(localhost:(3000|5173|8081)|sanal-(ogretmen|ogrenci)-ai(-.*)?\.vercel\.app)",
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

MODELS_TO_TRY = [
    "gemini-2.0-flash-exp",
    "gemini-1.5-flash",
    "gemini-1.5-pro",
]

MAX_FILE_SIZE = 6 * 1024 * 1024
ALLOWED_EXTENSIONS = {"jpg", "jpeg", "png", "webp"}
MIME_BY_EXT = {"jpg": "image/jpeg", "jpeg": "image/jpeg", "png": "image/png", "webp": "image/webp"}

# =======================================================
# 2) HELPER: GOOGLE CLOUD AUTH (RENDER Ä°Ã‡Ä°N)
# =======================================================
def ensure_gcp_credentials():
    """
    Render ortamÄ±nda Environment Variable'dan JSON key'i alÄ±r
    ve geÃ§ici bir dosyaya yazarak Google Vision'Ä±n kullanmasÄ±nÄ± saÄŸlar.
    """
    if os.getenv("GOOGLE_APPLICATION_CREDENTIALS"):
        return

    key_json = os.getenv("GCP_SA_KEY_JSON", "").strip()
    if not key_json:
        print("UYARI: GCP_SA_KEY_JSON bulunamadÄ±! Vision API Ã§alÄ±ÅŸmayabilir.")
        return

    try:
        path = "/tmp/gcp_sa.json"
        with open(path, "w", encoding="utf-8") as f:
            f.write(key_json)
        os.environ["GOOGLE_APPLICATION_CREDENTIALS"] = path
        print("âœ… Google Cloud Credentials baÅŸarÄ±yla yÃ¼klendi.")
    except Exception as e:
        print(f"âš ï¸ Credentials yÃ¼kleme hatasÄ±: {e}")

# =======================================================
# 3) HEALTH CHECK
# =======================================================
@app.get("/")
@app.get("/health")
def health_check():
    return {"status": "ok", "service": "Sanal Ogretmen AI Backend (Vision OCR + WordMask)"}

# =======================================================
# 4) DATA MODELS
# =======================================================
class AnalyzeRequest(BaseModel):
    ocr_text: str
    image_url: Optional[str] = ""
    student_name: str
    student_surname: str
    classroom_code: str
    level: str
    country: str
    native_language: str

class UpdateScoreRequest(BaseModel):
    submission_id: Union[int, str]
    new_rubric: dict
    new_total: int

# =======================================================
# 5) TDK & UTILS (Mevcut logic aynen korundu)
# =======================================================
def load_tdk_rules() -> List[Dict[str, Any]]:
    return [
        {"rule_id": "TDK_01_BAGLAC_DE", "text": "BaÄŸlaÃ§ olan 'da/de' ayrÄ± yazÄ±lÄ±r."},
        {"rule_id": "TDK_02_BAGLAC_KI", "text": "BaÄŸlaÃ§ olan 'ki' ayrÄ± yazÄ±lÄ±r."},
        {"rule_id": "TDK_03_SORU_EKI", "text": "Soru eki 'mÄ±/mi' ayrÄ± yazÄ±lÄ±r."},
        {"rule_id": "TDK_04_SEY_SOZ", "text": "'Åžey' sÃ¶zcÃ¼ÄŸÃ¼ daima ayrÄ± yazÄ±lÄ±r."},
        {"rule_id": "TDK_05_BUYUK_CUMLE", "text": "CÃ¼mleler bÃ¼yÃ¼k harfle baÅŸlar."},
        {"rule_id": "TDK_06_BUYUK_OZEL", "text": "Ã–zel isimler bÃ¼yÃ¼k harfle baÅŸlar."},
        {"rule_id": "TDK_08_BUYUK_GEREKSIZ", "text": "Ã–zel isim olmayan sÃ¶zcÃ¼kler cÃ¼mle iÃ§inde bÃ¼yÃ¼k harfle yazÄ±lamaz."},
        {"rule_id": "TDK_09_KESME_OZEL", "text": "Ã–zel isimlere gelen ekler kesme ile ayrÄ±lÄ±r (Samsun'a)."},
        {"rule_id": "TDK_13_KESME_GENEL", "text": "Cins isimlere gelen ekler kesme ile ayrÄ±lmaz (stadyuma, okula)."},
        {"rule_id": "TDK_12_SAYILAR", "text": "SayÄ±lar ayrÄ± yazÄ±lÄ±r (on beÅŸ)."},
        {"rule_id": "TDK_20_NOKTA", "text": "CÃ¼mle sonuna nokta konur."},
        {"rule_id": "TDK_21_VIRGUL", "text": "SÄ±ralÄ± kelimelere virgÃ¼l konur."},
        {"rule_id": "TDK_23_YANLIS_YALNIZ", "text": "YanlÄ±ÅŸ (yanÄ±lmak), YalnÄ±z (yalÄ±n)."},
        {"rule_id": "TDK_24_HERKES", "text": "Herkes (s ile)."},
        {"rule_id": "TDK_25_SERTLESME", "text": "SertleÅŸme kuralÄ± (Kitapta, 1923'te)."},
        {"rule_id": "TDK_28_YABANCI", "text": "YabancÄ± kelimeler (ÅžofÃ¶r, egzoz, makine)."}
    ]

_ZERO_WIDTH = re.compile(r"[\u200B\u200C\u200D\uFEFF]")
TR_LOWER_MAP = str.maketrans({"Ä°": "i", "I": "Ä±"})

def tr_lower(s: str) -> str:
    if not s: return ""
    return s.translate(TR_LOWER_MAP).lower()

def tr_lower_first(word: str) -> str:
    if not word: return ""
    return tr_lower(word[0]) + word[1:]

def normalize_text(text: str) -> str:
    if not text: return ""
    text = text.replace("â€™", "'").replace("`", "'")
    text = _ZERO_WIDTH.sub("", text)
    text = text.replace("\r\n", "\n").replace("\r", "\n")
    lines = [re.sub(r"[ \t]+", " ", ln).strip() for ln in text.split("\n")]
    lines = [ln for ln in lines if ln != ""]
    return "\n".join(lines).strip()

def normalize_match(text: str) -> str:
    return tr_lower(normalize_text(text))

def to_int(x, default=0):
    try:
        if x is None: return default
        if isinstance(x, (int, float)): return int(x)
        if isinstance(x, str):
            clean = re.sub(r"[^\d\-]", "", x.split("/")[0])
            return int(clean) if clean else default
        return default
    except:
        return default

async def read_limited(upload: UploadFile, limit: int) -> bytes:
    chunks = []
    size = 0
    while True:
        chunk = await upload.read(1024 * 1024)
        if not chunk: break
        size += len(chunk)
        if size > limit:
            raise HTTPException(status_code=413, detail=f"Dosya Ã§ok bÃ¼yÃ¼k (Maks {limit // (1024*1024)}MB).")
        chunks.append(chunk)
    return b"".join(chunks)

# =======================================================
# 5.1) OCR WORD-LEVEL RISK MASKING (YENÄ°)
# =======================================================
WORD_RE = re.compile(r"\b[^\W\d_]+\b", flags=re.UNICODE)

def mask_word(word: str, mask_char: str = "â°") -> str:
    # kelime uzunluÄŸu kadar â°
    return mask_char * len(word)

def make_risk_checks():
    """
    OCR'nin "emin olmadÄ±ÄŸÄ± halde yanlÄ±ÅŸ harf basmasÄ±nÄ±" yakalamak iÃ§in
    geniÅŸletilebilir kurallar. Burada kesin dÃ¼zeltme yok: sadece ÅŸÃ¼pheli kelimeyi
    tamamen â°â°â° yapÄ±yoruz.
    """
    # Ä°leride siz bÃ¼yÃ¼tebilirsiniz (Ã¶r: en sÄ±k geÃ§en iÅŸlev kelimeleri vs.)
    # Åžimdilik boÅŸ bÄ±rakÄ±yoruz; kural bazlÄ± yakalayÄ±cÄ±lar Ã§alÄ±ÅŸacak.
    RISK_WORDS = set()

    def in_risk_list(w: str) -> bool:
        return tr_lower(w) in RISK_WORDS

    # Karma bÃ¼yÃ¼k-kÃ¼Ã§Ã¼k (OCR'nin sÄ±k yaptÄ±ÄŸÄ±) => kelimeyi komple ÅŸÃ¼pheli say
    def weird_casing(w: str) -> bool:
        upp = sum(1 for ch in w if ch.isupper())
        low = sum(1 for ch in w if ch.islower())
        return upp >= 2 and low >= 1

    # TÃ¼rkÃ§e karakter ihtimali yÃ¼ksek olup ASCII sapmasÄ± gibi gÃ¶rÃ¼nen kelimeler.
    # Buraya kendi heuristiklerinizi ekleyebilirsiniz; ÅŸimdilik iskelet.
    def looks_tr_ascii_suspicious(w: str) -> bool:
        # Ã–rn: "cok", "cay" gibi kelimeler; listeyi siz zamanla geniÅŸletirsiniz.
        wl = tr_lower(w)
        return wl in {"cok", "cay"}

    return [in_risk_list, weird_casing, looks_tr_ascii_suspicious]

RISK_CHECKS = make_risk_checks()

def apply_word_level_risk_masking(text: str) -> str:
    def repl(m: re.Match) -> str:
        w = m.group(0)
        for check in RISK_CHECKS:
            try:
                if check(w):
                    return mask_word(w)
            except:
                continue
        return w
    return WORD_RE.sub(repl, text)

# =======================================================
# 6) ENDPOINTS
# =======================================================
@app.get("/check-class/{code}")
async def check_class_code(code: str):
    try:
        response = supabase.table("classrooms").select("name").eq("code", code.upper().strip()).execute()
        if response.data:
            return {"valid": True, "class_name": response.data[0]["name"]}
        return {"valid": False}
    except:
        return {"valid": False}

# =======================================================
# OCR: GOOGLE VISION (PROD READY - SECRET FILES UYUMLU)
# =======================================================
@app.post("/ocr")
async def ocr_image(file: UploadFile = File(...), classroom_code: str = Form(...)):
    try:
        # Render ortamÄ± iÃ§in credential ayarla (gÃ¼venli: varsa dokunmaz)
        ensure_gcp_credentials()

        file_content = await read_limited(file, MAX_FILE_SIZE)

        # ---------------------------------------------------
        # A) Dosya HazÄ±rlÄ±ÄŸÄ± ve Supabase Upload
        # ---------------------------------------------------
        filename = file.filename or "unknown.jpg"
        file_ext = filename.rsplit(".", 1)[-1].lower() if "." in filename else "jpg"
        if file_ext not in ALLOWED_EXTENSIONS:
            file_ext = "jpg"

        safe_mime = file.content_type or MIME_BY_EXT.get(file_ext, "image/jpeg")

        safe_code = re.sub(r"[^A-Za-z0-9_-]", "_", classroom_code)[:20]
        unique_filename = f"{safe_code}_{uuid.uuid4()}.{file_ext}"
        image_url = ""

        try:
            supabase.storage.from_("odevler").upload(
                unique_filename,
                file_content,
                {"content-type": safe_mime, "upsert": "false"},
            )
            res = supabase.storage.from_("odevler").get_public_url(unique_filename)
            image_url = res if isinstance(res, str) else res.get("publicUrl")
        except Exception:
            pass

        # ---------------------------------------------------
        # B) VISION API - BAÄžLANTI
        # ---------------------------------------------------
        try:
            vision_client = vision.ImageAnnotatorClient()
        except Exception as e:
            print(f"Vision Client HatasÄ±: {e}")
            return {
                "status": "error",
                "message": "Google Vision Yetkilendirme HatasÄ±. Secret Files ayarlÄ± mÄ±?",
            }

        image = vision.Image(content=file_content)

        # (Ä°steÄŸe baÄŸlÄ± ama faydalÄ±) TÃ¼rkÃ§e ipucu:
        # context = vision.ImageContext(language_hints=["tr"])
        # response = vision_client.document_text_detection(image=image, image_context=context)

        response = vision_client.document_text_detection(image=image)

        if response.error.message:
            return {"status": "error", "message": f"Vision API HatasÄ±: {response.error.message}"}

        # ---------------------------------------------------
        # C) CONFIDENCE FILTERING
        #   - Noktalama ASLA maskelenmez
        #   - Mask sadece HARF iÃ§in Ã§alÄ±ÅŸÄ±r
        # ---------------------------------------------------
        CONFIDENCE_THRESHOLD = 0.40

        masked_parts: list[str] = []
        raw_parts: list[str] = []

        PUNCTUATION = set(".,;:!?\"'â€™`()-â€“â€”â€¦")

        def is_letter(ch: str) -> bool:
            return bool(ch) and ch.isalpha()

        def is_punct(ch: str) -> bool:
            return ch in PUNCTUATION

        def append_break(break_type_val: int) -> None:
            if not break_type_val:
                return
            if break_type_val in (1, 2):
                masked_parts.append(" ")
                raw_parts.append(" ")
            elif break_type_val in (3, 5):
                masked_parts.append("\n")
                raw_parts.append("\n")

        for page in response.full_text_annotation.pages:
            for block in page.blocks:
                for paragraph in block.paragraphs:
                    for word in paragraph.words:
                        for symbol in word.symbols:
                            char = symbol.text or ""
                            conf = getattr(symbol, "confidence", 1.0)

                            raw_parts.append(char)

                            if is_punct(char):
                                masked_parts.append(char)
                            elif is_letter(char):
                                if conf < CONFIDENCE_THRESHOLD:
                                    masked_parts.append("â°")
                                else:
                                    masked_parts.append(char)
                            else:
                                masked_parts.append(char)

                            prop = getattr(symbol, "property", None)
                            db = getattr(prop, "detected_break", None) if prop else None
                            if db:
                                b_type = getattr(db, "type_", getattr(db, "type", 0))
                                append_break(int(b_type) if b_type else 0)

        raw_text = unicodedata.normalize("NFC", "".join(raw_parts).strip())
        masked_text = unicodedata.normalize("NFC", "".join(masked_parts).strip())

        # ---------------------------------------------------
        # D) WORD-LEVEL RISK MASKING (YENÄ°)
        #   - Kesin DÃœZELTME YOK
        #   - ÅžÃ¼pheli kelimeyi komple â°â°â° yap
        # ---------------------------------------------------
        masked_text = apply_word_level_risk_masking(masked_text)

        return {
            "status": "success",
            "ocr_text": masked_text,
            "raw_ocr_text": raw_text,
            "image_url": image_url,
            "ocr_notice": (
                f"â„¹ï¸ HARF confidence %{int(CONFIDENCE_THRESHOLD*100)} altÄ±ndaysa 'â°' basÄ±lÄ±r. "
                f"AyrÄ±ca riskli kelimeler word-level â°â°â° ile maskelenir. Noktalama asla maskelenmez."
            ),
            "ocr_markers": {"char": "â°", "word": "â°"},
        }

    except Exception as e:
        print(f"Sistem HatasÄ±: {e}")
        return {"status": "error", "message": f"Sunucu HatasÄ±: {str(e)}"}

# =======================================================
# ANALYZE: GEMINI (ANALÄ°Z VE PUANLAMA) - DEÄžÄ°ÅžMEDÄ°
# =======================================================
@app.post("/analyze")
async def analyze_submission(data: AnalyzeRequest):
    if not data.ocr_text or not data.ocr_text.strip():
        raise HTTPException(status_code=400, detail="Metin boÅŸ, analiz yapÄ±lamaz.")

    if "â°" in data.ocr_text:
        raise HTTPException(
            status_code=400,
            detail="OCR belirsiz (â°) iÅŸaretli yerler var. LÃ¼tfen Ã¶nce bu kÄ±sÄ±mlarÄ± dÃ¼zeltin."
        )

    full_text = normalize_text(data.ocr_text)
    display_text = full_text.replace("\n", " ")

    print(f"ðŸ§  Analiz: {data.student_name} ({data.level})")

    tdk_rules = load_tdk_rules()
    allowed_ids = {r["rule_id"] for r in tdk_rules}
    rules_text = "\n".join([f"- {r['rule_id']}: {r['text']}" for r in tdk_rules])

    prompt_tdk = f"""
ROL: Sen nesnel ve kuralcÄ± bir TDK denetÃ§isisin.
GÃ–REV: Metindeki yazÄ±m / noktalama / bÃ¼yÃ¼k-kÃ¼Ã§Ã¼k harf / kesme iÅŸareti / ek yazÄ±mÄ± hatalarÄ±nÄ± bul.
METÄ°N: \"\"\"{display_text}\"\"\"

REFERANS KURALLAR:
{rules_text}

Ã‡IKTI (SADECE JSON):
{{ "rubric_part": {{ "noktalama": 0, "dil_bilgisi": 0 }}, "errors": [] }}
"""

    prompt_cefr = f"""
ROL: Sen destekleyici bir Ã¶ÄŸretmensin.
GÃ–REV: {data.level} seviyesindeki Ã¶ÄŸrencinin iletiÅŸim becerisini deÄŸerlendir.
KURALLAR: YazÄ±m hatalarÄ±nÄ± gÃ¶z ardÄ± et, iletiÅŸime odaklan.
METÄ°N: \"\"\"{display_text}\"\"\"
Ã‡IKTI (JSON): {{ "rubric_part": {{ "uzunluk": 0, "soz_dizimi": 0, "kelime": 0, "icerik": 0 }}, "teacher_note": "..." }}
"""

    # --- Analyze devamÄ±: sizin mevcut kodunuzla aynÄ± kalmalÄ± ---
    # BurayÄ± sizdeki eski main.py devamÄ±yla aynen birleÅŸtirin.
    # (Ã–nceki sÃ¼rÃ¼mde paylaÅŸtÄ±ÄŸÄ±nÄ±z analyze bloÄŸunu aynen koruyun.)
    raise HTTPException(status_code=501, detail="Analyze devamÄ± bu kÄ±saltÄ±lmÄ±ÅŸ Ã¶rnekte yer almÄ±yor. Eski analyze bloÄŸunuzu buraya aynen yapÄ±ÅŸtÄ±rÄ±n.")


    final_result = None
    last_error = ""

    for model_name in MODELS_TO_TRY:
        try:
            # 1. TDK ANALÄ°ZÄ°
            resp_tdk = client.models.generate_content(
                model=model_name, contents=prompt_tdk,
                config=types.GenerateContentConfig(response_mime_type="application/json", temperature=0)
            )
            raw_tdk = (resp_tdk.text or "").strip()
            json_tdk = json.loads(raw_tdk.replace("```json", "").replace("```", "")) if raw_tdk else {}

            # 2. CEFR ANALÄ°ZÄ°
            resp_cefr = client.models.generate_content(
                model=model_name, contents=prompt_cefr,
                config=types.GenerateContentConfig(response_mime_type="application/json", temperature=0)
            )
            raw_cefr = (resp_cefr.text or "").strip()
            json_cefr = json.loads(raw_cefr.replace("```json", "").replace("```", "")) if raw_cefr else {}

            tdk_p = json_tdk.get("rubric_part", {})
            cefr_p = json_cefr.get("rubric_part", {})
            if not cefr_p: cefr_p = cefr_fallback_scores(data.level, full_text)

            combined_rubric = {
                "noktalama": min(14, max(0, to_int(tdk_p.get("noktalama")))),
                "dil_bilgisi": min(16, max(0, to_int(tdk_p.get("dil_bilgisi")))),
                "uzunluk": min(16, max(0, to_int(cefr_p.get("uzunluk")))),
                "soz_dizimi": min(20, max(0, to_int(cefr_p.get("soz_dizimi")))),
                "kelime": min(14, max(0, to_int(cefr_p.get("kelime")))),
                "icerik": min(20, max(0, to_int(cefr_p.get("icerik")))),
            }
            total_score = sum(combined_rubric.values())

            cleaned_tdk = validate_analysis(json_tdk, full_text, allowed_ids)
            rule_caps = find_unnecessary_capitals(full_text)
            rule_common = find_common_a2_errors(full_text)
            rule_dade = find_conjunction_dade_joined(full_text)

            all_errors = merge_and_dedupe_errors(cleaned_tdk.get("errors", []), rule_caps, rule_common, rule_dade)
            all_errors = pick_best_per_span(all_errors)

            errors_student, errors_ocr = [], []
            for e in all_errors:
                span = e.get("span") or {}
                if "start" not in span or "end" not in span: continue
                ocr_flag = bool(e.get("ocr_suspect", False)) or looks_like_ocr_noise(e.get("wrong", ""), full_text, span)
                if ocr_flag:
                    e["type"] = "OCR_ÅžÃœPHELÄ°"
                    e["explanation"] = (e.get("explanation", "") + " (OCR parÃ§alanmasÄ± olabilir.)").strip()
                    e["ocr_suspect"] = True
                    errors_ocr.append(e)
                else:
                    errors_student.append(e)
            
            errors_student.sort(key=lambda x: x["span"]["start"])
            errors_ocr.sort(key=lambda x: x["span"]["start"])

            raw_note = (json_cefr.get("teacher_note") or "").strip()
            if not raw_note: raw_note = f"[SEVÄ°YE: {data.level}] Not oluÅŸturulamadÄ±."
            elif not raw_note.startswith("["): raw_note = f"[SEVÄ°YE: {data.level}] " + raw_note

            final_result = {
                "rubric": combined_rubric,
                "errors": errors_student,
                "errors_student": errors_student,
                "errors_ocr": errors_ocr,
                "teacher_note": raw_note,
                "score_total": total_score
            }
            break
        except Exception as e:
            last_error = str(e)
            continue

    if not final_result:
        raise HTTPException(status_code=500, detail=f"Analiz baÅŸarÄ±sÄ±z: {last_error}")

    try:
        supabase.table("submissions").insert({
            "student_name": data.student_name.strip(),
            "student_surname": data.student_surname.strip(),
            "classroom_code": data.classroom_code.strip(),
            "image_url": data.image_url,
            "ocr_text": full_text,
            "level": data.level,
            "country": data.country,
            "native_language": data.native_language,
            "analysis_json": final_result,
            "score_total": final_result["score_total"]
        }).execute()
        return {"status": "success", "data": final_result}
    except Exception as e:
        print(f"DB KayÄ±t HatasÄ±: {e}")
        return {"status": "success", "data": final_result, "warning": "VeritabanÄ± hatasÄ±"}

@app.post("/student-history")
async def get_student_history(student_name: str = Form(...), student_surname: str = Form(...), classroom_code: str = Form(...)):
    try:
        response = supabase.table("submissions").select("*")\
            .ilike("student_name", student_name.strip())\
            .ilike("student_surname", student_surname.strip())\
            .eq("classroom_code", classroom_code.strip())\
            .order("created_at", desc=True).execute()
        return {"status": "success", "data": response.data}
    except Exception as e:
        return {"status": "error", "message": str(e)}

@app.post("/update-score")
async def update_score(data: UpdateScoreRequest):
    try:
        res = supabase.table("submissions").select("analysis_json").eq("id", data.submission_id).execute()
        if not res.data: raise HTTPException(status_code=404, detail="KayÄ±t bulunamadÄ±")
        
        current_json = res.data[0].get("analysis_json") or {}
        if "rubric" not in current_json: current_json["rubric"] = {}
        current_json["rubric"].update(data.new_rubric)

        supabase.table("submissions").update({
            "score_total": data.new_total,
            "analysis_json": current_json
        }).eq("id", data.submission_id).execute()
        return {"status": "success", "message": "Puan gÃ¼ncellendi"}
    except HTTPException: raise
    except Exception as e: raise HTTPException(status_code=500, detail=str(e))