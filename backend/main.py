from fastapi import FastAPI, UploadFile, File, Form, HTTPException
from fastapi.middleware.cors import CORSMiddleware
from google import genai
from google.genai import types
from google.cloud import vision
from supabase import create_client, Client
from dotenv import load_dotenv
import os, json, uuid, re
import unicodedata
from pydantic import BaseModel
from typing import Union, List, Dict, Any, Optional

# =======================================================
# 1) AYARLAR VE KURULUM
# =======================================================
load_dotenv()

API_KEY = os.getenv("GEMINI_API_KEY")
if not API_KEY:
    raise RuntimeError("‚ùå KRƒ∞Tƒ∞K HATA: GEMINI_API_KEY eksik!")

SUPABASE_URL = (os.getenv("SUPABASE_URL", "") or "").rstrip("/")
SUPABASE_KEY = os.getenv("SUPABASE_KEY")
if not SUPABASE_URL or not SUPABASE_KEY:
    raise RuntimeError("‚ùå KRƒ∞Tƒ∞K HATA: SUPABASE bilgileri eksik!")

client = genai.Client(api_key=API_KEY)
supabase: Client = create_client(SUPABASE_URL, SUPABASE_KEY)

app = FastAPI(title="Sanal Ogretmen AI API", version="4.2.0 (Bug Fixes)")

app.add_middleware(
    CORSMiddleware,
    allow_origin_regex=r"https?://(localhost:(3000|5173|8081)|sanal-(ogretmen|ogrenci)-ai(-.*)?\.vercel\.app)",
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

MODELS_TO_TRY = ["gemini-2.0-flash", "gemini-1.5-flash"]
MAX_FILE_SIZE = 6 * 1024 * 1024
ALLOWED_EXTENSIONS = {"jpg", "jpeg", "png", "webp"}
MIME_BY_EXT = {"jpg": "image/jpeg", "jpeg": "image/jpeg", "png": "image/png", "webp": "image/webp"}

# =======================================================
# 2) HELPER: GOOGLE CLOUD AUTH
# =======================================================
def ensure_gcp_credentials():
    if os.getenv("GOOGLE_APPLICATION_CREDENTIALS"): return
    key_json = os.getenv("GCP_SA_KEY_JSON", "").strip()
    if not key_json: return
    try:
        path = "/tmp/gcp_sa.json"
        with open(path, "w", encoding="utf-8") as f: f.write(key_json)
        os.environ["GOOGLE_APPLICATION_CREDENTIALS"] = path
    except Exception as e: print(f"‚ö†Ô∏è Credentials hatasƒ±: {e}")

# =======================================================
# 3) DATA MODELS
# =======================================================
class AnalyzeRequest(BaseModel):
    ocr_text: str
    image_url: Optional[str] = ""
    student_name: str
    student_surname: str
    classroom_code: str
    level: str
    country: str
    native_language: str

class UpdateScoreRequest(BaseModel):
    submission_id: Union[int, str]
    new_rubric: dict
    new_total: int

# =======================================================
# 4) TEXT & TDK UTILS
# =======================================================

def load_tdk_rules() -> List[Dict[str, Any]]:
    return [
        {"rule_id": "TDK_01_BAGLAC_DE", "text": "Baƒüla√ß olan 'da/de' ayrƒ± yazƒ±lƒ±r."},
        {"rule_id": "TDK_02_BAGLAC_KI", "text": "Baƒüla√ß olan 'ki' ayrƒ± yazƒ±lƒ±r."},
        {"rule_id": "TDK_03_SORU_EKI_MI", "text": "Soru eki 'mƒ±/mi' ayrƒ± yazƒ±lƒ±r."},
        {"rule_id": "TDK_04_SEY_AYRI", "text": "'≈ûey' s√∂zc√ºƒü√º daima ayrƒ± yazƒ±lƒ±r."},
        {"rule_id": "TDK_06_YA_DA", "text": "'Ya da' ayrƒ± yazƒ±lƒ±r."},
        {"rule_id": "TDK_07_HER_SEY", "text": "'Her ≈üey' ayrƒ± yazƒ±lƒ±r."},
        {"rule_id": "TDK_12_GEREKSIZ_BUYUK", "text": "C√ºmle i√ßinde gereksiz b√ºy√ºk harf kullanƒ±lmaz."},
        {"rule_id": "TDK_30_NOKTA_CUMLE_SONU", "text": "C√ºmle sonuna uygun noktalama i≈üareti konur (genelde nokta)."},
        {"rule_id": "TDK_20_KESME_OZEL_AD", "text": "√ñzel isimlere gelen ekler kesme ile ayrƒ±lƒ±r."},
        {"rule_id": "TDK_23_KESME_GENEL_YOK", "text": "Cins isimlere gelen ekler kesme ile ayrƒ±lmaz."},
        {"rule_id": "TDK_40_COK", "text": "'√áok' kelimesinin yazƒ±mƒ±."},
        {"rule_id": "TDK_41_HERKES", "text": "'Herkes' (s ile yazƒ±lƒ±r)."},
        {"rule_id": "TDK_42_YALNIZ", "text": "'Yalnƒ±z' (yalƒ±n k√∂k√ºnden)."},
        {"rule_id": "TDK_43_YANLIS", "text": "'Yanlƒ±≈ü' (yanƒ±lmak k√∂k√ºnden)."},
        {"rule_id": "TDK_44_BIRKAC", "text": "'Birka√ß' biti≈üik yazƒ±lƒ±r."},
        {"rule_id": "TDK_45_HICBIR", "text": "'Hi√ßbir' biti≈üik yazƒ±lƒ±r."},
        {"rule_id": "TDK_46_PEKCOK", "text": "'Pek √ßok' ayrƒ± yazƒ±lƒ±r."},
        {"rule_id": "TDK_47_INSALLAH", "text": "'ƒ∞n≈üallah' kelimesinin yazƒ±mƒ±."},
        {"rule_id": "TDK_31_SORU_ISARETI", "text": "Soru c√ºmlesi soru i≈üareti (?) ile biter."},

    ]

SEVERITY_BY_RULE = {
    "TDK_12_GEREKSIZ_BUYUK": "MINOR",
    "TDK_30_NOKTA_CUMLE_SONU": "MINOR",
    "TDK_40_COK": "MAJOR",
    "TDK_01_BAGLAC_DE": "MAJOR",
    "TDK_02_BAGLAC_KI": "MAJOR",
    "TDK_03_SORU_EKI_MI": "MAJOR",
    "TDK_04_SEY_AYRI": "MAJOR",
    "TDK_06_YA_DA": "MAJOR",
    "TDK_07_HER_SEY": "MAJOR",
    "TDK_23_KESME_GENEL_YOK": "MAJOR",
    "TDK_41_HERKES": "MAJOR",
    "TDK_42_YALNIZ": "MAJOR",
    "TDK_43_YANLIS": "MAJOR",
    "TDK_44_BIRKAC": "MAJOR",
    "TDK_45_HICBIR": "MAJOR",
    "TDK_46_PEKCOK": "MAJOR",
    "TDK_47_INSALLAH": "MAJOR",
    "TDK_31_SORU_ISARETI": "MAJOR"

}

_ZERO_WIDTH = re.compile(r"[\u200B\u200C\u200D\uFEFF]")
TR_LOWER_MAP = str.maketrans({"ƒ∞": "i", "I": "ƒ±"})

def tr_lower(s: str) -> str:
    if not s: return ""
    return s.translate(TR_LOWER_MAP).lower()

def tr_lower_first(word: str) -> str:
    if not word: return ""
    return tr_lower(word[0]) + word[1:]

def normalize_text(text: str) -> str:
    if not text: return ""
    text = text.replace("‚Äô", "'").replace("`", "'")
    text = _ZERO_WIDTH.sub("", text)
    text = text.replace("\r\n", "\n").replace("\r", "\n")
    lines = [re.sub(r"[ \t]+", " ", ln).strip() for ln in text.split("\n")]
    lines = [ln for ln in lines if ln != ""]
    return "\n".join(lines).strip()

def normalize_match(text: str) -> str:
    return tr_lower(normalize_text(text))

def to_int(x, default=0):
    try:
        if x is None: return default
        if isinstance(x, (int, float)): return int(x)
        if isinstance(x, str):
            clean = re.sub(r"[^\d\-]", "", x.split("/")[0])
            return int(clean) if clean else default
        return default
    except: return default

async def read_limited(upload: UploadFile, limit: int) -> bytes:
    chunks = []
    size = 0
    while True:
        chunk = await upload.read(1024 * 1024)
        if not chunk: break
        size += len(chunk)
        if size > limit: raise HTTPException(status_code=413, detail="Dosya √ßok b√ºy√ºk.")
        chunks.append(chunk)
    return b"".join(chunks)

SENT_BOUNDARY = re.compile(r"([.!?]+|[\n\r]+|[:;]+|‚Äî|‚Äì|-{2,})")
def sentence_starts(text: str) -> set:
    starts = {0}
    for m in SENT_BOUNDARY.finditer(text):
        idx = m.end()
        while idx < len(text) and text[idx].isspace(): idx += 1
        if idx < len(text): starts.add(idx)
    return starts

_SENT_END = re.compile(r"[.!?\n\r]+")
def _has_question_mark_in_same_sentence(full_text: str, idx: int) -> bool:
    if not full_text: return False
    left = idx
    while left > 0 and not _SENT_END.match(full_text[left - 1]): left -= 1
    right = idx
    n = len(full_text)
    while right < n and not _SENT_END.match(full_text[right]): right += 1
    return "?" in full_text[left:right]

PROPER_ROOTS = {"samsun", "karadeniz", "t√ºrkiye", "piazza", "city", "mall", "meydan", "sahil", "avm", "tramvay"}
COMMON_SUFFIXES = ("dan","den","tan","ten","da","de","ta","te","a","e")

def norm_token(token: str) -> str:
    if not token: return ""
    t = token.strip().replace("‚Äô", "'")
    t = re.sub(r"[.,;:!?()\[\]{}]", "", t)
    return t

def strip_common_suffixes(root: str) -> str:
    r = root
    for suf in sorted(COMMON_SUFFIXES, key=len, reverse=True):
        if r.endswith(suf) and len(r) > len(suf) + 2:
            return r[:-len(suf)]
    return r

def token_root(token: str) -> str:
    t = norm_token(token)
    if "'" in t: t = t.split("'")[0]
    r = tr_lower(t)
    r = strip_common_suffixes(r) 
    return r

def is_probably_proper(word: str) -> bool:
    r = token_root(word)
    if r in PROPER_ROOTS: return True
    if "'" in norm_token(word) and word[:1].isupper(): return True
    return False

def _find_best_span(full_text: str, wrong: str, hint_start: int = None):
    wrong_n = normalize_match(wrong).replace("\n", " ")
    full_n = normalize_match(full_text).replace("\n", " ")
    if not wrong_n: return None
    matches = []
    start_idx = 0
    while True:
        idx = full_n.find(wrong_n, start_idx)
        if idx == -1: break
        matches.append(idx)
        start_idx = idx + 1
    if not matches: return None
    best = min(matches, key=lambda x: abs(x - hint_start)) if hint_start is not None else matches[0]
    return (best, best + len(wrong_n))

# --- OCR VE G√úVENLƒ∞K YARDIMCILARI ---
# EKSƒ∞K OLAN FONKSƒ∞YON BURAYA EKLENDƒ∞:
OCR_NOISE_PATTERNS = [re.compile(r".*\b[a-zA-Zƒü√º≈ü√∂√ßƒ±ƒ∞ƒû√ú≈û√ñ√á]+['‚Äô][a-zA-Z]\b"), re.compile(r"^[a-zA-Z]\b")]
def looks_like_ocr_noise(wrong: str, full_text: str, span: dict) -> bool:
    w = (wrong or "").strip()
    if len(w) <= 1: return True
    for p in OCR_NOISE_PATTERNS:
        if p.search(w):
            if " " in w and len(w.split()) == 2 and len(w.split()[1]) == 1: return True
    return False

# --- DETERMINISTIK TDK FONKSIYONLARI ---
# =======================================================
# TDK_31: SORU C√úMLESƒ∞ SORU ƒ∞≈ûARETƒ∞ (?)
# =======================================================

def _split_sentences_with_spans(text: str):
    """
    Geli≈ütirilmi≈ü segmentleyici:
    - . ! ? ve satƒ±r sonlarƒ±
    - ayrƒ±ca virg√ºlden sonra gelen soru kelimesi varsa yeni segment
    """
    parts = []
    start = 0
    n = len(text)

    for m in SENT_BOUNDARY.finditer(text):
        end = m.end()
        parts.append((text[start:end], start, end))
        start = end

    if start < n:
        parts.append((text[start:n], start, n))

    # ƒ∞Kƒ∞NCƒ∞ GE√áƒ∞≈û: virg√ºl + soru kelimesi
    final_parts = []
    for seg, s0, s1 in parts:
        last_cut = 0
        for m in re.finditer(r",\s*(?=(ne|neden|ni√ßin|nicin|nasƒ±l|nasil|kim|hangi|ka√ß|kac|nerede|nereye|nereden|ne\s*zaman)\b)", seg, flags=re.IGNORECASE | re.UNICODE):
            cut = m.start() + 1
            final_parts.append((seg[last_cut:cut], s0 + last_cut, s0 + cut))
            last_cut = cut
        final_parts.append((seg[last_cut:], s0 + last_cut, s1))
    return final_parts


def _is_question_like(seg: str) -> bool:
    """
    Soru gibi g√∂r√ºnen c√ºmleyi yakala:
    - soru kelimeleri: ne, neden, ni√ßin, nasƒ±l, kim, hangi, ka√ß, nerede, nereye, nereden, ne zaman
    - soru eki: mƒ±/mi/mu/m√º (ayrƒ± veya birle≈üik)
    """
    s = tr_lower(seg.strip())
    if not s:
        return False

    # zaten soru i≈üareti varsa => eksik deƒüil
    if "?" in seg:
        return False

    # soru kelimeleri
    if re.search(r"\b(ne|neden|ni√ßin|nicin|nasƒ±l|nasil|kim|hangi|ka√ß|kac|nerede|nereye|nerden|nereden|ne\s*zaman)\b", s, flags=re.UNICODE):
        return True

    # ayrƒ± yazƒ±lmƒ±≈ü soru eki
    if re.search(r"\b(mƒ±|mi|mu|m√º)\b", s, flags=re.UNICODE):
        return True

    # birle≈üik yazƒ±lmƒ±≈ü soru eki (geliyormusun vb.)
    if re.search(r"\b[^\W\d_]{2,}(mƒ±|mi|mu|m√º)\b", s, flags=re.UNICODE):
        return True

    return False

def _last_word_span_in_segment(seg: str, global_start: int):
    """
    Segment i√ßindeki son kelimenin global span'ƒ±nƒ± d√∂nd√ºr√ºr.
    """
    last = None
    for m in re.finditer(r"\b[^\W\d_]+\b", seg, flags=re.UNICODE):
        last = m
    if not last:
        return None
    return (global_start + last.start(), global_start + last.end())

def find_missing_question_mark(full_text: str) -> list:
    """
    Soru gibi g√∂r√ºnen segmentlerde '?' yoksa i≈üaretle.
    """
    errs = []
    if not full_text:
        return errs

    for seg, s0, s1 in _split_sentences_with_spans(full_text):
        if not seg or not seg.strip():
            continue

        if not _is_question_like(seg):
            continue

        # span: son kelimeyi i≈üaretleyelim
        wspan = _last_word_span_in_segment(seg, s0)
        if not wspan:
            continue
        ws, we = wspan
        wrong = full_text[ws:we]

        errs.append({
            "wrong": wrong,
            "correct": f"{wrong}?",
            "type": "Noktalama",
            "rule_id": "TDK_31_SORU_ISARETI",
            "explanation": "Soru c√ºmleleri soru i≈üareti (?) ile biter.",
            "span": {"start": ws, "end": we},
            "ocr_suspect": False,
            "suggestion_type": "FIX",
            "confidence": 0.80
        })

    return errs

_MI_JOINED = re.compile(r"\b([^\W\d_]{2,})(mƒ±|mi|mu|m√º)\b", flags=re.UNICODE | re.IGNORECASE)
_MI_FALSE_WORDS = {"kimi", "≈üimdi", "simdi", "resmi", "ismi", "yemi", "temi"}
def find_soru_eki_mi_joined(full_text: str) -> list:
    errs = []
    if not full_text: return errs
    for m in _MI_JOINED.finditer(full_text):
        whole = full_text[m.start():m.end()]
        base, mi = m.group(1), m.group(2)
        if tr_lower(whole) in _MI_FALSE_WORDS: continue
        if "'" in whole or "‚Äô" in whole: continue
        
        correct = f"{base} {mi}"
        has_q = _has_question_mark_in_same_sentence(full_text, m.start())
        if has_q:
            errs.append({"wrong": whole, "correct": correct, "type": "Yazƒ±m", "rule_id": "TDK_03_SORU_EKI_MI", "explanation": "Soru eki ayrƒ± yazƒ±lƒ±r.", "span": {"start": m.start(), "end": m.end()}, "ocr_suspect": True, "suggestion_type": "FIX", "confidence": 0.92})
        else:
            errs.append({"wrong": whole, "correct": correct, "type": "OCR_≈û√úPHELƒ∞", "rule_id": "TDK_03_SORU_EKI_MI", "explanation": "Soru eki biti≈üik yazƒ±lmƒ±≈ü olabilir (≈ü√ºpheli).", "span": {"start": m.start(), "end": m.end()}, "ocr_suspect": True, "suggestion_type": "FLAG", "confidence": 0.55})
    return errs

_KI_JOINED = re.compile(r"\b([^\W\d_]{3,})(ki)\b", flags=re.UNICODE | re.IGNORECASE)
_KI_VERBISH_ENDINGS = ("yorum", "iyorum", "ƒ±yorum", "uyorum", "yorsun", "yor", "yordu", "yorlar", "dƒ±m", "dim", "dum", "d√ºm", "tƒ±m", "tim", "tum", "t√ºm", "dƒ±n", "din", "dun", "d√ºn", "tƒ±n", "tin", "tun", "t√ºn", "dƒ±", "di", "du", "d√º", "tƒ±", "ti", "tu", "t√º", "mƒ±≈ü", "mi≈ü", "mu≈ü", "m√º≈ü", "acak", "ecek", "acaƒüƒ±m", "eceƒüim", "acaksƒ±n", "eceksin", "malƒ±", "meli", "malƒ±dƒ±r", "melidir")
_KI_BLACKLIST = {"d√ºnk√º", "bug√ºnk√º", "yarƒ±nki", "≈üimdiki", "sonraki", "evvelki", "√∂nceki"}
def find_baglac_ki_joined(full_text: str) -> list:
    errs = []
    if not full_text: return errs
    for m in _KI_JOINED.finditer(full_text):
        whole, base, ki = full_text[m.start():m.end()], m.group(1), m.group(2)
        if "'" in whole or "‚Äô" in whole: continue
        if tr_lower(whole) in _KI_BLACKLIST: continue
        if not any(tr_lower(base).endswith(end) for end in _KI_VERBISH_ENDINGS): continue
        errs.append({"wrong": whole, "correct": f"{base} {ki}", "type": "Yazƒ±m", "rule_id": "TDK_02_BAGLAC_KI", "explanation": "Baƒüla√ß olan 'ki' ayrƒ± yazƒ±lƒ±r.", "span": {"start": m.start(), "end": m.end()}, "ocr_suspect": True, "suggestion_type": "FIX", "confidence": 0.85})
    return errs

_SEY_JOINED = re.compile(r"\b([^\W\d_]{1,10})≈üey\b", flags=re.UNICODE | re.IGNORECASE)
_SEY_PREFIX_OK = {"bir", "hi√ßbir", "hicbir", "≈üu", "su", "bu", "o", "b√∂yle", "boyle"}
def find_sey_joined(full_text: str) -> list:
    errs = []
    if not full_text: return errs
    for m in _SEY_JOINED.finditer(full_text):
        whole, prefix = full_text[m.start():m.end()], m.group(1)
        if tr_lower(whole) in {"her≈üey", "hersey"}: continue
        if "'" in whole or "‚Äô" in whole: continue
        if tr_lower(prefix) not in _SEY_PREFIX_OK: continue
        errs.append({"wrong": whole, "correct": f"{prefix} ≈üey", "type": "Yazƒ±m", "rule_id": "TDK_04_SEY_AYRI", "explanation": "'≈ûey' s√∂zc√ºƒü√º ayrƒ± yazƒ±lƒ±r.", "span": {"start": m.start(), "end": m.end()}, "ocr_suspect": True, "suggestion_type": "FIX", "confidence": 0.93})
    return errs

_HERSEY = re.compile(r"\b(her≈üey|hersey)\b", flags=re.UNICODE | re.IGNORECASE)
def find_hersey_joined(full_text: str) -> list:
    errs = []
    for m in _HERSEY.finditer(full_text):
        whole = full_text[m.start():m.end()]
        errs.append({"wrong": whole, "correct": "her ≈üey", "type": "Yazƒ±m", "rule_id": "TDK_07_HER_SEY", "explanation": "'Her ≈üey' ayrƒ± yazƒ±lƒ±r.", "span": {"start": m.start(), "end": m.end()}, "ocr_suspect": True, "suggestion_type": "FIX", "confidence": 0.95})
    return errs

_YADA = re.compile(r"\b(yada|ya-da|ya‚Äìda|ya‚Äîda)\b", flags=re.UNICODE | re.IGNORECASE)
def find_yada_joined(full_text: str) -> list:
    errs = []
    for m in _YADA.finditer(full_text):
        whole = full_text[m.start():m.end()]
        errs.append({"wrong": whole, "correct": "ya da", "type": "Yazƒ±m", "rule_id": "TDK_06_YA_DA", "explanation": "'Ya da' ayrƒ± yazƒ±lƒ±r.", "span": {"start": m.start(), "end": m.end()}, "ocr_suspect": True, "suggestion_type": "FIX", "confidence": 0.95})
    return errs

_BIR_KAC = re.compile(r"\bbir\s+k(a|√¢)√ß\b", flags=re.UNICODE | re.IGNORECASE)
def find_bir_kac_separated(full_text: str) -> list:
    errs = []
    for m in _BIR_KAC.finditer(full_text):
        whole = full_text[m.start():m.end()]
        errs.append({"wrong": whole, "correct": "birka√ß", "type": "Yazƒ±m", "rule_id": "TDK_44_BIRKAC", "explanation": "'Birka√ß' biti≈üik yazƒ±lƒ±r.", "span": {"start": m.start(), "end": m.end()}, "ocr_suspect": True, "suggestion_type": "FIX", "confidence": 0.92})
    return errs

_HIC_BIR = re.compile(r"\bhi√ß\s+bir\b", flags=re.UNICODE | re.IGNORECASE)
def find_hic_bir_separated(full_text: str) -> list:
    errs = []
    for m in _HIC_BIR.finditer(full_text):
        whole = full_text[m.start():m.end()]
        errs.append({"wrong": whole, "correct": "hi√ßbir", "type": "Yazƒ±m", "rule_id": "TDK_45_HICBIR", "explanation": "'Hi√ßbir' biti≈üik yazƒ±lƒ±r.", "span": {"start": m.start(), "end": m.end()}, "ocr_suspect": True, "suggestion_type": "FIX", "confidence": 0.93})
    return errs

_PEKCOK_JOINED = re.compile(r"\bpek√ßok\b", flags=re.UNICODE | re.IGNORECASE)

def find_pekcok_joined(full_text: str) -> list:
    errs = []
    for m in _PEKCOK_JOINED.finditer(full_text or ""):
        whole = full_text[m.start():m.end()]
        errs.append({
            "wrong": whole, "correct": "pek √ßok", "type": "Yazƒ±m",
            "rule_id": "TDK_46_PEKCOK", "explanation": "'Pek √ßok' ayrƒ± yazƒ±lƒ±r.",
            "span": {"start": m.start(), "end": m.end()},
            "ocr_suspect": True, "suggestion_type": "FIX", "confidence": 0.95
        })
    return errs

def find_common_misspellings(full_text: str) -> list:
    errs = []
    patterns = [
        (re.compile(r"\bherkez\b", re.IGNORECASE | re.UNICODE), "herkes", "TDK_41_HERKES", "'Herkes' (s ile yazƒ±lƒ±r)."),
        (re.compile(r"\byanliz\b", re.IGNORECASE | re.UNICODE), "yalnƒ±z", "TDK_42_YALNIZ", "'Yalnƒ±z' kelimesinin yazƒ±mƒ±."),
        (re.compile(r"\byanlis\b", re.IGNORECASE | re.UNICODE), "yanlƒ±≈ü", "TDK_43_YANLIS", "'Yanlƒ±≈ü' kelimesinin yazƒ±mƒ±."),
        (re.compile(r"\binsallah\b", re.IGNORECASE | re.UNICODE), "in≈üallah", "TDK_47_INSALLAH", "'ƒ∞n≈üallah' kelimesinin yazƒ±mƒ±."),
    ]
    for rx, correct, rid, expl in patterns:
        for m in rx.finditer(full_text):
            whole = full_text[m.start():m.end()]
            errs.append({"wrong": whole, "correct": correct, "type": "Yazƒ±m", "rule_id": rid, "explanation": expl, "span": {"start": m.start(), "end": m.end()}, "ocr_suspect": True, "suggestion_type": "FIX", "confidence": 0.95})
    return errs

# EKSƒ∞K OLAN POSSESSIVE_HINT BURAYA EKLENDƒ∞:
POSSESSIVE_HINT = re.compile(r"(ƒ±m|im|um|√ºm|ƒ±n|in|un|√ºn|m|n)$", re.IGNORECASE | re.UNICODE)

def find_conjunction_dade_joined(full_text: str) -> list:
    errs = []
    for m in re.finditer(r"\b([^\W\d_]+)(da|de)\b", full_text, flags=re.UNICODE | re.IGNORECASE):
        base, suf = m.group(1), m.group(2)
        whole = full_text[m.start():m.end()]
        if POSSESSIVE_HINT.search(base): continue
        if any(ch.isupper() for ch in whole) or is_probably_proper(whole): continue
        errs.append({"wrong": whole, "correct": f"{base} {suf}", "type": "Yazƒ±m", "rule_id": "TDK_01_BAGLAC_DE", "explanation": "Baƒüla√ß olan da/de ayrƒ± yazƒ±lƒ±r.", "span": {"start": m.start(), "end": m.end()}, "ocr_suspect": True, "suggestion_type": "FIX", "confidence": 0.85})
    return errs

def find_common_a2_errors(full_text: str) -> list:
    errs = []
    for m in re.finditer(r"\b(cok|√ßog|c√∂k|coK|COk|sok)\b", full_text, flags=re.IGNORECASE):
        errs.append({"wrong": m.group(0), "correct": "√ßok", "type": "Yazƒ±m", "rule_id": "TDK_40_COK", "explanation": "‚Äò√ßok‚Äô kelimesinin yazƒ±mƒ±.", "span": {"start": m.start(), "end": m.end()}, "ocr_suspect": True, "suggestion_type": "FIX", "confidence": 0.95})
    return errs

def find_unnecessary_capitals(full_text: str) -> list:
    starts = sentence_starts(full_text)
    errors = []
    for m in re.finditer(r"\b[^\W\d_]+\b", full_text, flags=re.UNICODE):
        word = m.group(0)
        s, e = m.start(), m.end()
        if s in starts: continue
        if is_probably_proper(word): continue
        if tr_lower(word) in {"sok"}: continue
        
        upp = sum(1 for ch in word if ch.isupper())
        low = sum(1 for ch in word if ch.islower())
        if (upp >= 2 and low >= 1):
            errors.append({"wrong": word, "correct": word, "type": "OCR_≈û√úPHELƒ∞", "rule_id": "TDK_12_GEREKSIZ_BUYUK", "explanation": "B√ºy√ºk/k√º√ß√ºk harf karƒ±≈üƒ±klƒ±ƒüƒ± OCR kaynaklƒ± olabilir.", "span": {"start": s, "end": e}, "ocr_suspect": True, "suggestion_type": "FLAG", "confidence": 0.5})
            continue
        if word and word[0].isupper():
            errors.append({"wrong": word, "correct": tr_lower_first(word), "type": "B√ºy√ºk Harf", "rule_id": "TDK_12_GEREKSIZ_BUYUK", "explanation": "C√ºmle ortasƒ±nda gereksiz b√ºy√ºk harf kullanƒ±mƒ±.", "span": {"start": s, "end": e}, "ocr_suspect": False, "suggestion_type": "FIX", "confidence": 0.9})
    return errors

# --- LLM SERT Fƒ∞LTRE ---
def _only_case_change(wrong: str, correct: str) -> bool: return normalize_match(wrong) == normalize_match(correct) and wrong != correct
def _only_apostrophe_remove(wrong: str, correct: str) -> bool: return normalize_text(wrong).replace("'", "") == normalize_text(correct)
def _only_adds_space_for_mi(wrong: str, correct: str) -> bool:
    w, c = normalize_match(wrong), normalize_match(correct)
    return w and c and c.replace(" ", "") == w and (" " in c)

def _is_safe_tdk_pair(rule_id: str, wrong: str, correct: str, full_text: str, span: dict) -> bool:
    w = normalize_text(wrong)
    c = normalize_text(correct)
    s = to_int((span or {}).get("start"), None)
    e = to_int((span or {}).get("end"), None)

    # Genel g√ºvenlik: "uydurma" gibi g√∂r√ºnen b√ºy√ºk deƒüi≈üimleri engelle
    # (√ßok uzun kelime d√∂n√º≈ü√ºmleri vs.)
    if not w or not c:
        return False

    # 1) MI: sadece bo≈üluk ekleme + soru c√ºmlesi i√ßinde olmalƒ±
    if rule_id == "TDK_03_SORU_EKI_MI":
        if not _only_adds_space_for_mi(w, c):
            return False
        if s is not None and e is not None and not _has_question_mark_in_same_sentence(full_text, s):
            return False
        return True

    # 2) B√ºy√ºk harf: sadece harf b√ºy√ºkl√ºƒü√º deƒüi≈üsin
    if rule_id == "TDK_12_GEREKSIZ_BUYUK":
        return _only_case_change(w, c)

    # 3) da/de baƒüla√ß: sadece bo≈üluk ekleme (biti≈üik -> ayrƒ±)
    if rule_id == "TDK_01_BAGLAC_DE":
        wn = normalize_match(w)
        cn = normalize_match(c)
        return cn.replace(" ", "") == wn and (" " in cn)

    # 4) ki baƒüla√ß: sadece bo≈üluk ekleme (biti≈üik -> ayrƒ±)
    if rule_id == "TDK_02_BAGLAC_KI":
        wn = normalize_match(w)
        cn = normalize_match(c)
        return cn.replace(" ", "") == wn and (" " in cn)

    # 5) ≈üey: sadece "X≈üey" -> "X ≈üey" gibi bo≈üluk ekleme
    if rule_id == "TDK_04_SEY_AYRI":
        wn = normalize_match(w)
        cn = normalize_match(c)
        return cn.replace(" ", "") == wn and (" " in cn)

    # 6) her ≈üey: sadece doƒüru forma d√∂ns√ºn
    if rule_id == "TDK_07_HER_SEY":
        return normalize_match(c) == "her ≈üey" and normalize_match(w) in {"her≈üey", "hersey"}

    # 7) ya da: sadece doƒüru forma d√∂ns√ºn
    if rule_id == "TDK_06_YA_DA":
        return normalize_match(c) == "ya da" and normalize_match(w).replace("‚Äî", "-").replace("‚Äì", "-") in {"yada", "ya-da"}

    # 8) birka√ß / hi√ßbir / pek √ßok: sadece hedef doƒüru yazƒ±ma d√∂ns√ºn
    if rule_id == "TDK_44_BIRKAC":
        return normalize_match(c) == "birka√ß"
    if rule_id == "TDK_45_HICBIR":
        return normalize_match(c) == "hi√ßbir"
    if rule_id == "TDK_46_PEKCOK":
        return normalize_match(c) == "pek √ßok"

    # 9) herkes/yalnƒ±z/yanlƒ±≈ü/in≈üallah gibi sƒ±k yanlƒ±≈ülar: sadece hedef doƒüru kelimeye d√∂ns√ºn
    if rule_id in {"TDK_41_HERKES", "TDK_42_YALNIZ", "TDK_43_YANLIS", "TDK_47_INSALLAH"}:
        # Bu kurallarda "correct" tek kelimelik sabit bir d√ºzeltme olmalƒ±
        return len(c.split()) == 1 and len(w.split()) == 1 and len(c) <= 15

    # 10) Kesme: sadece apostrof kaldƒ±rma
    if rule_id == "TDK_23_KESME_GENEL_YOK":
        return _only_apostrophe_remove(w, c)

    # 11) √ßok: sadece "√ßok" olsun
    if rule_id == "TDK_40_COK":
        return normalize_match(c) == "√ßok"

    # Diƒüerleri: g√ºvenli deƒüil => reddet
    return False

def validate_analysis(result: Dict[str, Any], full_text: str, allowed_ids: set) -> Dict[str, Any]:
    if not isinstance(result, dict): return {"errors": []}
    clean_errors = []
    for err in result.get("errors", []):
        if not isinstance(err, dict): continue
        rid = err.get("rule_id")
        if rid not in allowed_ids: continue
        wrong, correct = err.get("wrong", "") or "", err.get("correct", "") or ""
        if not wrong or not correct: continue
        
        hint = None
        if isinstance(err.get("span"), dict): hint = to_int(err["span"].get("start"), None)
        fixed = _find_best_span(full_text, wrong, hint)
        
        if fixed:
            start, end = fixed
            temp_span = {"start": start, "end": end}
            # LLM HATALARI ƒ∞√áƒ∞N SERT Fƒ∞LTRE
            if not _is_safe_tdk_pair(rid, wrong, correct, full_text, temp_span): continue
            
            clean_errors.append({
                "wrong": wrong, "correct": correct, "type": "Yazƒ±m",
                "rule_id": rid, "explanation": err.get("explanation", ""),
                "span": temp_span, "ocr_suspect": bool(err.get("ocr_suspect", False)),
                "suggestion_type": "FIX", "confidence": 0.85, "severity": SEVERITY_BY_RULE.get(rid, "MINOR")
            })
    clean_errors.sort(key=lambda x: x["span"]["start"])
    return {"errors": clean_errors}

def merge_and_dedupe_errors(*lists: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
    seen, merged = set(), []
    for lst in lists:
        for e in (lst or []):
            sp = e.get("span", {}) or {}
            key = (sp.get("start"), sp.get("end"), e.get("rule_id"))
            if key in seen: continue
            seen.add(key)
            merged.append(e)
    merged.sort(key=lambda x: x.get("span", {}).get("start", 10**9))
    return merged

def pick_best_per_span(errors: list) -> list:
    buckets = {}
    for e in errors:
        sp = e.get("span") or {}
        key = (sp.get("start"), sp.get("end"))
        if None in key: continue
        buckets.setdefault(key, []).append(e)
    chosen = []
    for _, items in buckets.items():
        # √ñncelik sƒ±rasƒ± eklenebilir, ≈üimdilik basit√ße ilki
        # FLAG vs FIX durumunda FIX √∂ncelikli olabilir
        best = max(items, key=lambda x: 10 if x.get("suggestion_type") == "FIX" else 5)
        chosen.append(best) 
    chosen.sort(key=lambda x: x["span"]["start"])
    return chosen

def cefr_fallback_scores(level: str, text: str) -> Dict[str, int]:
    t = normalize_text(text).replace("\n", " ")
    if not t: return {"uzunluk": 0, "soz_dizimi": 0, "kelime": 0, "icerik": 0}
    words = re.findall(r"\b[^\W\d_]+\b", t, flags=re.UNICODE)
    sentences = [s for s in re.split(r"[.!?]+", t) if s.strip()]
    has_connectors = bool(re.search(r"\b(ve|ama|√ß√ºnk√º|bu y√ºzden|sonra|fakat)\b", tr_lower(t)))
    uniq = len(set([tr_lower(w) for w in words])) if words else 0
    uzunluk = min(16, max(4, int(len(words) / 10) + 6))
    kelime = min(14, max(5, int(uniq / 8) + 6))
    soz = 8
    if len(sentences) >= 3: soz += 4
    if has_connectors: soz += 4
    soz_dizimi = min(20, max(6, soz))
    icerik = 8
    if len(sentences) >= 3: icerik += 4
    if len(words) >= 40: icerik += 4
    icerik = min(20, max(6, icerik))
    return {"uzunluk": int(uzunluk), "soz_dizimi": int(soz_dizimi), "kelime": int(kelime), "icerik": int(icerik)}

# =======================================================
# 6) ENDPOINTS
# =======================================================
@app.get("/check-class/{code}")
async def check_class_code(code: str):
    try:
        response = supabase.table("classrooms").select("name").eq("code", code.upper().strip()).execute()
        if response.data: return {"valid": True, "class_name": response.data[0]["name"]}
        return {"valid": False}
    except: return {"valid": False}

@app.post("/ocr")
async def ocr_image(file: UploadFile = File(...), classroom_code: str = Form(...)):
    try:
        ensure_gcp_credentials()
        file_content = await read_limited(file, MAX_FILE_SIZE)
        
        filename = f"{uuid.uuid4()}.jpg"
        image_url = ""
        try:
            supabase.storage.from_("odevler").upload(filename, file_content, {"content-type": "image/jpeg"})
            image_url = supabase.storage.from_("odevler").get_public_url(filename)
        except: pass

        try: vision_client = vision.ImageAnnotatorClient()
        except: return {"status": "error", "message": "Vision API Hatasƒ±"}

        image = vision.Image(content=file_content)
        context = vision.ImageContext(language_hints=["tr"])
        response = vision_client.document_text_detection(image=image, image_context=context)
        if response.error.message: return {"status": "error", "message": response.error.message}

        CONFIDENCE_THRESHOLD = 0.40
        masked_parts, raw_parts = [], []
        PUNCTUATION = set(".,;:!?\"'‚Äô`()-‚Äì‚Äî‚Ä¶")

        def append_break(break_type_val: int):
            if not break_type_val: return
            if break_type_val in (1, 2):
                masked_parts.append(" "); raw_parts.append(" ")
            elif break_type_val in (3, 5):
                masked_parts.append("\n"); raw_parts.append("\n")

        for page in response.full_text_annotation.pages:
            for block in page.blocks:
                for paragraph in block.paragraphs:
                    for word in paragraph.words:
                        for symbol in word.symbols:
                            ch = symbol.text or ""
                            conf = getattr(symbol, "confidence", 1.0)
                            raw_parts.append(ch)
                            if ch in PUNCTUATION: masked_parts.append(ch)
                            elif ch.isalpha(): masked_parts.append("‚ç∞" if conf < CONFIDENCE_THRESHOLD else ch)
                            else: masked_parts.append(ch)
                            prop = getattr(symbol, "property", None)
                            db = getattr(prop, "detected_break", None) if prop else None
                            if db: append_break(int(getattr(db, "type_", getattr(db, "type", 0))))

        raw_text = unicodedata.normalize("NFC", "".join(raw_parts).strip())
        masked_text = unicodedata.normalize("NFC", "".join(masked_parts).strip())

        def force_suspect(t: str) -> str:
            t = re.sub(r"\b[gG]ok\b", lambda m: "‚ç∞"+m.group(0)[1:], t)
            return re.sub(r"\b[gG]ay\b", lambda m: "‚ç∞"+m.group(0)[1:], t)
        
        masked_text = force_suspect(masked_text)

        return {"status": "success", "ocr_text": masked_text, "raw_ocr_text": raw_text, "image_url": image_url}
    except Exception as e: return {"status": "error", "message": str(e)}

@app.post("/analyze")
async def analyze_submission(data: AnalyzeRequest):
    if not data.ocr_text or not data.ocr_text.strip():
        raise HTTPException(status_code=400, detail="Metin bo≈ü.")
    if "‚ç∞" in data.ocr_text:
        raise HTTPException(status_code=400, detail="√ñnce ‚ç∞ i≈üaretlerini d√ºzeltin.")

    full_text = normalize_text(data.ocr_text)
    display_text = full_text.replace("\n", " ")

    print(f"üß† Analiz: {data.student_name} ({data.level})")

    # 1. A≈ûAMA: TDK ANALƒ∞Zƒ∞ (ƒ∞zinli Rule ID Listesi)
    tdk_rules = load_tdk_rules()
    allowed_ids = {r["rule_id"] for r in tdk_rules}
    rules_text = "\n".join([f"- {r['rule_id']}: {r['text']}" for r in tdk_rules])

    # Kƒ±sƒ±tlƒ± LLM Prompt: Sadece izinli hatalarƒ± bul, uydurma.
    prompt_tdk = f"""
    ROL: Sen TDK denet√ßisisin.
    G√ñREV: Metindeki yazƒ±m hatalarƒ±nƒ± SADECE a≈üaƒüƒ±daki kural setine g√∂re bul.
    ASLA metinde olmayan kelimeleri uydurma (Hallucination yapma).
    ASLA kelimenin k√∂k√ºn√º deƒüi≈ütirme (√ñrn: mont -> mantƒ± YAPMA).
    
    REFERANS KURALLAR (SADECE BUNLARA BAK):
    {rules_text}

    METƒ∞N: \"\"\"{display_text}\"\"\"
    √áIKTI (JSON): {{ "errors": [ {{ "wrong": "...", "correct": "...", "rule_id": "...", "explanation": "..." }} ] }}
    """

    # 2. A≈ûAMA: CEFR PUANLAMA
    prompt_rubric = f"""
    ROL: √ñƒüretmen ({data.level}).
    METƒ∞N: \"\"\"{display_text}\"\"\"
    
    PUANLA (TOPLAM 100):
    1. UZUNLUK (0-16)
    2. NOKTALAMA (0-14)
    3. Dƒ∞L Bƒ∞LGƒ∞Sƒ∞ (0-16)
    4. S√ñZ Dƒ∞Zƒ∞Mƒ∞ (0-20)
    5. KELƒ∞ME (0-14)
    6. ƒ∞√áERƒ∞K (0-20)

    √áIKTI: {{ "rubric_part": {{ "uzunluk": 0, "noktalama": 0, "dil_bilgisi": 0, "soz_dizimi": 0, "kelime": 0, "icerik": 0 }}, "teacher_note": "..." }}
    """

    final_result = None
    
    for model_name in MODELS_TO_TRY:
        try:
            # TDK
            resp_tdk = client.models.generate_content(
                model=model_name, contents=prompt_tdk,
                config=types.GenerateContentConfig(response_mime_type="application/json", temperature=0)
            )
            json_tdk = json.loads(resp_tdk.text.strip().replace("```json", "").replace("```", "")) if resp_tdk.text else {}

            # Rubric
            resp_rubric = client.models.generate_content(
                model=model_name, contents=prompt_rubric,
                config=types.GenerateContentConfig(response_mime_type="application/json", temperature=0.1)
            )
            json_rubric = json.loads(resp_rubric.text.strip().replace("```json", "").replace("```", "")) if resp_rubric.text else {}

            # Puanlar (Fallback ile g√ºvenli hale getirildi)
            p = json_rubric.get("rubric_part", {})
            fb = cefr_fallback_scores(data.level, full_text)
            
            def safe_score(key, max_val):
                val = to_int(p.get(key))
                if val == 0: val = fb.get(key, 0) # Fallback kullan
                return min(max_val, max(0, val))

            combined_rubric = {
                "uzunluk": safe_score("uzunluk", 16),
                "noktalama": safe_score("noktalama", 14),
                "dil_bilgisi": safe_score("dil_bilgisi", 16),
                "soz_dizimi": safe_score("soz_dizimi", 20),
                "kelime": safe_score("kelime", 14),
                "icerik": safe_score("icerik", 20),
            }
            total_score = sum(combined_rubric.values())

            # Hata ƒ∞≈üleme (LLM + Deterministik Regex Birle≈üimi)
            cleaned_tdk = validate_analysis(json_tdk, full_text, allowed_ids) # LLM hatalarƒ± (filtrelenmi≈ü)
            
            # Deterministik TDK fonksiyonlarƒ±
            rule_caps = find_unnecessary_capitals(full_text)
            rule_common = find_common_a2_errors(full_text)
            rule_dade = find_conjunction_dade_joined(full_text)
            rule_ki = find_baglac_ki_joined(full_text)
            rule_sey = find_sey_joined(full_text)
            rule_hersey = find_hersey_joined(full_text)
            rule_yada = find_yada_joined(full_text)
            rule_birkac = find_bir_kac_separated(full_text)
            rule_hicbir = find_hic_bir_separated(full_text)
            rule_pekcok = find_pekcok_joined(full_text)
            rule_mi = find_soru_eki_mi_joined(full_text) # FIX + FLAG stratejisi
            rule_miss = find_common_misspellings(full_text)
            rule_qmark = find_missing_question_mark(full_text)


            all_errors = merge_and_dedupe_errors(
                cleaned_tdk.get("errors", []),
                rule_caps, rule_common, rule_dade,
                rule_ki, rule_sey, rule_hersey, rule_yada,
                rule_birkac, rule_hicbir, rule_pekcok, rule_mi, rule_miss,
                rule_qmark
            )
            all_errors = pick_best_per_span(all_errors)

            # Severity ekle (backend'den renk y√∂netimi i√ßin)
            for e in all_errors:
                e.setdefault("severity", SEVERITY_BY_RULE.get(e.get("rule_id"), "MINOR"))
                e.setdefault("confidence", 0.85)
                e.setdefault("suggestion_type", "FIX")

            # OCR vs √ñƒürenci Ayrƒ±mƒ±
            errors_student, errors_ocr = [], []
            for e in all_errors:
                span = e.get("span") or {}
                if "start" not in span: continue
                ocr_flag = looks_like_ocr_noise(e.get("wrong", ""), full_text, span)
                if ocr_flag:
                    e["type"] = "OCR_≈û√úPHELƒ∞"
                    e["ocr_suspect"] = True
                    e["suggestion_type"] = "FLAG" 
                    errors_ocr.append(e)
                else:
                    e["type"] = "Yazƒ±m"
                    e["ocr_suspect"] = False
                    e["suggestion_type"] = "FIX"
                    errors_student.append(e)
            
            errors_student.sort(key=lambda x: x["span"]["start"])

            final_result = {
                "rubric": combined_rubric,
                "errors": errors_student,
                "errors_student": errors_student,
                "errors_ocr": errors_ocr,
                "teacher_note": json_rubric.get("teacher_note", "Deƒüerlendirme tamamlandƒ±."),
                "score_total": total_score
            }
            break
        except Exception as e:
            print(f"Hata ({model_name}): {e}")
            continue

    if not final_result: raise HTTPException(status_code=500, detail="Analiz yapƒ±lamadƒ±.")

    try:
        supabase.table("submissions").insert({
            "student_name": data.student_name, "student_surname": data.student_surname,
            "classroom_code": data.classroom_code, "image_url": data.image_url,
            "ocr_text": full_text, "level": data.level, "country": data.country,
            "native_language": data.native_language, "analysis_json": final_result,
            "score_total": final_result["score_total"]
        }).execute()
        return {"status": "success", "data": final_result}
    except Exception: return {"status": "success", "data": final_result, "warning": "DB Hatasƒ±"}

@app.post("/student-history")
async def get_student_history(student_name: str = Form(...), student_surname: str = Form(...), classroom_code: str = Form(...)):
    try:
        res = supabase.table("submissions").select("*")\
            .ilike("student_name", student_name.strip())\
            .ilike("student_surname", student_surname.strip())\
            .eq("classroom_code", classroom_code.strip())\
            .order("created_at", desc=True).execute()
        return {"status": "success", "data": res.data}
    except Exception as e: return {"status": "error", "message": str(e)}

@app.post("/update-score")
async def update_score(data: UpdateScoreRequest):
    try:
        res = supabase.table("submissions").select("analysis_json").eq("id", data.submission_id).execute()
        if not res.data: raise HTTPException(status_code=404, detail="Kayƒ±t yok")
        curr = res.data[0]["analysis_json"]
        if "rubric" not in curr: curr["rubric"] = {}
        curr["rubric"].update(data.new_rubric)
        supabase.table("submissions").update({ "score_total": data.new_total, "analysis_json": curr }).eq("id", data.submission_id).execute()
        return {"status": "success", "message": "G√ºncellendi"}
    except Exception as e: raise HTTPException(status_code=500, detail=str(e))