from fastapi import FastAPI, UploadFile, File, Form, HTTPException
from fastapi.middleware.cors import CORSMiddleware
from google import genai
from google.genai import types
from google.cloud import vision
from supabase import create_client, Client
from dotenv import load_dotenv
import os, json, uuid, re
import unicodedata
from pydantic import BaseModel
from typing import Union, List, Dict, Any, Optional
import asyncio

# =======================================================
# 1) AYARLAR VE KURULUM
# =======================================================
load_dotenv()

API_KEY = os.getenv("GEMINI_API_KEY")
if not API_KEY:
    raise RuntimeError("‚ùå KRƒ∞Tƒ∞K HATA: GEMINI_API_KEY eksik!")

SUPABASE_URL = (os.getenv("SUPABASE_URL", "") or "").rstrip("/")
SUPABASE_KEY = os.getenv("SUPABASE_KEY")
if not SUPABASE_URL or not SUPABASE_KEY:
    raise RuntimeError("‚ùå KRƒ∞Tƒ∞K HATA: SUPABASE bilgileri eksik!")

client = genai.Client(api_key=API_KEY)
supabase: Client = create_client(SUPABASE_URL, SUPABASE_KEY)

app = FastAPI(title="Sanal Ogretmen AI API - TUBITAK Hybrid Edition", version="5.0.0")

app.add_middleware(
    CORSMiddleware,
    allow_origin_regex=r"https?://(localhost:(3000|5173|8081)|sanal-(ogretmen|ogrenci)-ai(-.*)?\.vercel\.app)",
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

MODELS_TO_TRY = ["gemini-2.0-flash", "gemini-1.5-flash"]
MAX_FILE_SIZE = 6 * 1024 * 1024

# =======================================================
# 2) AKADEMƒ∞K REFERANS VERƒ∞ SETLERƒ∞ VE REGEX (DESENLER)
# =======================================================

# Yanlƒ±≈ü pozitifleri engellemek i√ßin istisna listeleri
MI_SUFFIX_BLACKLIST = {
    "cami", "mami", "hami", "samimi", "kimi", "t√ºm√º", "ilhami", "resmi", "cismi",
    "ismi", "yemi", "gemisi", "sevgilisi", "kendisi", "annesi", "babasƒ±", "abisi",
    "mermi", "irmi", "vermi", "gemi", "komi"
}

PROPER_NOUNS_WHITELIST = {
    "t√ºrkiye", "samsun", "istanbul", "ankara", "izmir", "atat√ºrk", "mehmet", "ahmet",
    "ay≈üe", "fatma", "ali", "veli", "atakum", "ilkadƒ±m", "canik", "√ßar≈üamba", "bafra",
    "ingilizce", "t√ºrk√ße", "almanca", "fransƒ±zca", "allah", "tanrƒ±"
}

# Regex Kalƒ±plarƒ± (Word Boundary \b ile kesin e≈üle≈üme saƒülar)
PATTERNS = {
    # Kural: "mi" soru eki her zaman ayrƒ± yazƒ±lƒ±r. (Kelimeye biti≈üik mi/mƒ±/mu/m√º varsa yakalar)
    "TDK_03_SORU_EKI": re.compile(r"\b(\w{2,})(mi|mƒ±|mu|m√º)(?=[?.!,;:\s]|$)", re.IGNORECASE | re.UNICODE),
    
    # Kural: "≈üey" her zaman ayrƒ± yazƒ±lƒ±r. (bir≈üey, her≈üey, √ßok≈üey...)
    "TDK_04_SEY_AYRI": re.compile(r"\b(\w+)≈üey\b", re.IGNORECASE | re.UNICODE),
    
    # Kural: "ya da" ayrƒ± yazƒ±lƒ±r. (yada)
    "TDK_06_YA_DA": re.compile(r"\byada\b", re.IGNORECASE | re.UNICODE),
    
    # Kural: "her ≈üey" ayrƒ± yazƒ±lƒ±r.
    "TDK_07_HER_SEY": re.compile(r"\bher≈üey\b", re.IGNORECASE | re.UNICODE),
    
    # Kural: "birka√ß", "hi√ßbir" biti≈üik; "pek √ßok" ayrƒ±.
    "TDK_44_BIRKAC": re.compile(r"\bbir\s+ka√ß\b", re.IGNORECASE | re.UNICODE),
    "TDK_45_HICBIR": re.compile(r"\bhi√ß\s+bir\b", re.IGNORECASE | re.UNICODE),
    "TDK_46_PEKCOK": re.compile(r"\bpek√ßok\b", re.IGNORECASE | re.UNICODE),
    
    # Kural: Sƒ±k yapƒ±lan imla hatalarƒ±
    "TDK_41_HERKES": re.compile(r"\bherkez\b", re.IGNORECASE | re.UNICODE),
    "TDK_42_YALNIZ": re.compile(r"\byanliz\b", re.IGNORECASE | re.UNICODE),
    "TDK_43_YANLIS": re.compile(r"\byanlis\b", re.IGNORECASE | re.UNICODE),
    "TDK_47_INSALLAH": re.compile(r"\binsallah\b", re.IGNORECASE | re.UNICODE),
    
    # Kural: Cins isimlere gelen ekler kesme ile ayrƒ±lmaz (Kitap'ƒ± -> Kitabƒ±)
    # B√ºy√ºk harfle ba≈ülamayan ama kesme i≈üareti i√ßeren kelimeler.
    "TDK_23_KESME_GENEL": re.compile(r"\b([a-z√ßƒüƒ±√∂≈ü√º]{3,})'([a-z√ßƒüƒ±√∂≈ü√º]+)\b", re.UNICODE)
}

# √ñzel ƒ∞sim Regex'i: B√ºy√ºk harfle ba≈üla + k√º√ß√ºk devam et + ek al + kesme yok.
# √ñrn: Ahmetin, Samsuna, T√ºrkiyeden
PROPER_NOUN_SUFFIX_REGEX = re.compile(
    r"\b([A-Z√áƒûƒ∞√ñ≈û√ú][a-z√ßƒüƒ±√∂≈ü√º]{2,})(nin|nƒ±n|nun|n√ºn|in|ƒ±n|un|√ºn|de|da|den|dan|e|a|i|ƒ±|u|√º|le|la)\b",
    re.UNICODE
)

# =======================================================
# 3) DATA MODELS & HELPERS
# =======================================================
class AnalyzeRequest(BaseModel):
    ocr_text: str
    image_url: Optional[str] = ""
    student_name: str
    student_surname: str
    classroom_code: str
    level: str
    country: str
    native_language: str

class UpdateScoreRequest(BaseModel):
    submission_id: Union[int, str]
    new_rubric: dict
    new_total: int

def normalize_text(text: str) -> str:
    if not text: return ""
    text = text.replace("‚Äô", "'").replace("`", "'")
    text = unicodedata.normalize("NFKC", text)
    return text.strip()

def tr_lower(text: str) -> str:
    return text.replace("ƒ∞", "i").replace("I", "ƒ±").lower()

def safe_json(text: str) -> dict:
    if not text: return {}
    t = text.strip().replace("```json", "").replace("```", "").strip()
    try: return json.loads(t)
    except: return {}

def to_int(x, default=0):
    try:
        if x is None: return default
        if isinstance(x, (int, float)): return int(x)
        if isinstance(x, str):
            clean = re.sub(r"[^\d\-]", "", x.split("/")[0])
            return int(clean) if clean else default
        return default
    except: return default

def get_sentence_starts(text: str) -> set:
    starts = {0}
    for match in re.finditer(r"[.!?]\s+", text):
        starts.add(match.end())
    return starts

async def ensure_gcp_credentials():
    if os.getenv("GOOGLE_APPLICATION_CREDENTIALS"): return
    key_json = os.getenv("GCP_SA_KEY_JSON", "").strip()
    if not key_json: return
    try:
        path = "/tmp/gcp_sa.json"
        with open(path, "w", encoding="utf-8") as f: f.write(key_json)
        os.environ["GOOGLE_APPLICATION_CREDENTIALS"] = path
    except: pass

async def read_limited(upload: UploadFile, limit: int) -> bytes:
    chunks = []
    size = 0
    while True:
        chunk = await upload.read(1024 * 1024)
        if not chunk: break
        size += len(chunk)
        if size > limit: raise HTTPException(status_code=413, detail="Dosya √ßok b√ºy√ºk.")
        chunks.append(chunk)
    return b"".join(chunks)

# =======================================================
# 4) CORE ALGORƒ∞TMA: DETERMƒ∞Nƒ∞STƒ∞K ANALƒ∞Z (REGEX)
# =======================================================
def analyze_deterministic(text: str) -> List[Dict[str, Any]]:
    errors = []
    
    # 1. STANDART REGEX HATALARI
    for rule_id, pattern in PATTERNS.items():
        for match in pattern.finditer(text):
            whole_word = match.group(0)
            
            # ƒ∞stisna Kontrol√º (MI Eki)
            if rule_id == "TDK_03_SORU_EKI":
                stem = match.group(1)
                suffix = match.group(2)
                if tr_lower(whole_word) in MI_SUFFIX_BLACKLIST:
                    continue # Cami, mermi gibi kelimeleri ge√ß
                correct = f"{stem} {suffix}"
                explanation = "Soru eki 'mi/mƒ±' her zaman ayrƒ± yazƒ±lƒ±r."
            
            elif rule_id == "TDK_04_SEY_AYRI":
                stem = match.group(1)
                correct = f"{stem} ≈üey"
                explanation = "'≈ûey' s√∂zc√ºƒü√º her zaman ayrƒ± yazƒ±lƒ±r."
                
            elif rule_id == "TDK_06_YA_DA": 
                correct = "ya da"
                explanation = "'Ya da' baƒülacƒ± ayrƒ± yazƒ±lƒ±r."
            elif rule_id == "TDK_07_HER_SEY": 
                correct = "her ≈üey"
                explanation = "'Her ≈üey' ayrƒ± yazƒ±lƒ±r."
            elif rule_id == "TDK_44_BIRKAC": 
                correct = "birka√ß"
                explanation = "'Birka√ß' kelimesi biti≈üik yazƒ±lƒ±r."
            elif rule_id == "TDK_45_HICBIR": 
                correct = "hi√ßbir"
                explanation = "'Hi√ßbir' kelimesi biti≈üik yazƒ±lƒ±r."
            elif rule_id == "TDK_46_PEKCOK": 
                correct = "pek √ßok"
                explanation = "'Pek √ßok' ayrƒ± yazƒ±lƒ±r."
            elif rule_id == "TDK_41_HERKES": 
                correct = "herkes"
                explanation = "'Herkes' kelimesi 's' ile yazƒ±lƒ±r."
            elif rule_id == "TDK_42_YALNIZ": 
                correct = "yalnƒ±z"
                explanation = "Yalƒ±n k√∂k√ºnden gelir, 'yalnƒ±z' yazƒ±lƒ±r."
            elif rule_id == "TDK_43_YANLIS": 
                correct = "yanlƒ±≈ü"
                explanation = "Yanƒ±lmak k√∂k√ºnden gelir, 'yanlƒ±≈ü' yazƒ±lƒ±r."
            elif rule_id == "TDK_47_INSALLAH": 
                correct = "in≈üallah"
                explanation = "Doƒüru yazƒ±m 'in≈üallah' ≈üeklindedir."
            
            elif rule_id == "TDK_23_KESME_GENEL":
                stem = match.group(1)
                suffix = match.group(2)
                # Basit yumu≈üatma tahmini (Kitap'ƒ± -> Kitabƒ±)
                correct = f"{stem}{suffix}"
                if stem.endswith("p") and suffix[0] in "aƒ±ou": correct = f"{stem[:-1]}b{suffix}"
                elif stem.endswith("t") and suffix[0] in "aƒ±ou": correct = f"{stem[:-1]}d{suffix}"
                elif stem.endswith("√ß") and suffix[0] in "aƒ±ou": correct = f"{stem[:-1]}c{suffix}"
                elif stem.endswith("k") and suffix[0] in "aƒ±ou": correct = f"{stem[:-1]}ƒü{suffix}"
                explanation = "Cins isimlere gelen ekler kesme i≈üaretiyle ayrƒ±lmaz."

            errors.append({
                "wrong": whole_word,
                "correct": correct,
                "rule_id": rule_id,
                "span": {"start": match.start(), "end": match.end()},
                "type": "Yazƒ±m",
                "explanation": explanation,
                "confidence": 1.0,
                "source": "RULE_BASED"
            })

    # 2. √ñZEL ƒ∞Sƒ∞M ANALƒ∞Zƒ∞ (Ahmetin -> Ahmet'in)
    sentence_starts = get_sentence_starts(text)
    
    for match in PROPER_NOUN_SUFFIX_REGEX.finditer(text):
        whole_word = match.group(0)
        stem = match.group(1)
        suffix = match.group(2)
        start_idx = match.start()
        
        is_sentence_start = start_idx in sentence_starts
        
        # Eƒüer c√ºmle ba≈üƒ± DEƒûƒ∞LSE ve B√ºy√ºk harfle ba≈ülƒ±yorsa -> Kesin √ñzel ƒ∞simdir.
        # Eƒüer c√ºmle ba≈üƒ±ysa ama bilinen ≈üehir/isim listesindeyse -> Yine √ñzel ƒ∞simdir.
        if (not is_sentence_start) or (tr_lower(stem) in PROPER_NOUNS_WHITELIST):
            errors.append({
                "wrong": whole_word,
                "correct": f"{stem}'{suffix}",
                "rule_id": "TDK_20_KESME_OZEL_AD",
                "span": {"start": match.start(), "end": match.end()},
                "type": "Noktalama",
                "explanation": "√ñzel isimlere gelen ekler kesme i≈üareti ile ayrƒ±lƒ±r.",
                "confidence": 0.95,
                "source": "RULE_BASED"
            })

    return errors

# =======================================================
# 5) ENDPOINTS
# =======================================================

@app.get("/check-class/{code}")
async def check_class_code(code: str):
    try:
        response = supabase.table("classrooms").select("name").eq("code", code.upper().strip()).execute()
        if response.data: return {"valid": True, "class_name": response.data[0]["name"]}
        return {"valid": False}
    except: return {"valid": False}

@app.post("/ocr")
async def ocr_image(file: UploadFile = File(...), classroom_code: str = Form(...)):
    try:
        await ensure_gcp_credentials()
        file_content = await read_limited(file, MAX_FILE_SIZE)
        
        filename = f"{uuid.uuid4()}.jpg"
        image_url = ""
        try:
            supabase.storage.from_("odevler").upload(filename, file_content, {"content-type": "image/jpeg"})
            image_url = supabase.storage.from_("odevler").get_public_url(filename)
        except: pass

        try: vision_client = vision.ImageAnnotatorClient()
        except: return {"status": "error", "message": "Vision API Hatasƒ±"}

        image = vision.Image(content=file_content)
        context = vision.ImageContext(language_hints=["tr"])
        response = vision_client.document_text_detection(image=image, image_context=context)
        if response.error.message: return {"status": "error", "message": response.error.message}

        CONFIDENCE_THRESHOLD = 0.40
        masked_parts, raw_parts = [], []
        PUNCTUATION = set(".,;:!?\"'‚Äô`()-‚Äì‚Äî‚Ä¶")

        def append_break(break_type_val: int):
            if not break_type_val: return
            if break_type_val in (1, 2):
                masked_parts.append(" "); raw_parts.append(" ")
            elif break_type_val in (3, 5):
                masked_parts.append("\n"); raw_parts.append("\n")

        for page in response.full_text_annotation.pages:
            for block in page.blocks:
                for paragraph in block.paragraphs:
                    for word in paragraph.words:
                        for symbol in word.symbols:
                            ch = symbol.text or ""
                            conf = getattr(symbol, "confidence", 1.0)
                            raw_parts.append(ch)
                            if ch in PUNCTUATION: masked_parts.append(ch)
                            elif ch.isalpha(): masked_parts.append("‚ç∞" if conf < CONFIDENCE_THRESHOLD else ch)
                            else: masked_parts.append(ch)
                            prop = getattr(symbol, "property", None)
                            db = getattr(prop, "detected_break", None) if prop else None
                            if db: append_break(int(getattr(db, "type_", getattr(db, "type", 0))))

        raw_text = unicodedata.normalize("NFC", "".join(raw_parts).strip())
        masked_text = unicodedata.normalize("NFC", "".join(masked_parts).strip())

        # ≈û√ºpheli OCR D√ºzeltmeleri
        def force_suspect(t: str) -> str:
            t = re.sub(r"\b[gG]ok\b", lambda m: "‚ç∞"+m.group(0)[1:], t)
            return re.sub(r"\b[gG]ay\b", lambda m: "‚ç∞"+m.group(0)[1:], t)
        
        masked_text = force_suspect(masked_text)

        return {"status": "success", "ocr_text": masked_text, "raw_ocr_text": raw_text, "image_url": image_url}
    except Exception as e: return {"status": "error", "message": str(e)}

@app.post("/analyze")
async def analyze_submission(data: AnalyzeRequest):
    if not data.ocr_text or not data.ocr_text.strip():
        raise HTTPException(status_code=400, detail="Metin bo≈ü.")
    if "‚ç∞" in data.ocr_text:
        raise HTTPException(status_code=400, detail="√ñnce ‚ç∞ i≈üaretlerini d√ºzeltin.")

    # 1. Metni Normalize Et (Unicode NFC)
    full_text = normalize_text(data.ocr_text)
    
    print(f"üß† Hƒ∞BRƒ∞T ANALƒ∞Z BA≈ûLIYOR: {data.student_name} ({data.level})")

    # --- A≈ûAMA 1: Deterministik (Kural Tabanlƒ± - Regex) Analiz ---
    # Bu a≈üama hƒ±zlƒ±dƒ±r, kesindir ve kayma yapmaz.
    rule_errors = analyze_deterministic(full_text)
    
    # --- A≈ûAMA 2: LLM (Yapay Zeka) Analizi ---
    # Regex'in yakalayamadƒ±ƒüƒ± anlamsal bozukluklar i√ßin (glince -> gelince vb.)
    
    # Prompt: LLM'e sadece Regex'in bulamadƒ±ƒüƒ± ≈üeyleri bulmasƒ±nƒ± s√∂yl√ºyoruz.
    prompt = f"""
    G√ñREV: A≈üaƒüƒ±daki √∂ƒürenci metnini analiz et ve TDK kurallarƒ±na g√∂re hatalarƒ± bul.
    
    √ñNEMLƒ∞ KURALLAR:
    1. Zaten bildiƒüin -de/-da, -ki, mi/mƒ±, √∂zel isim kesme hatalarƒ±nƒ± yoksayabilirsin (bunlarƒ± regex ile bulduk).
    2. Odaklanman gereken: "glince" -> "gelince" gibi kelime bozukluklarƒ± veya anlatƒ±m bozukluklarƒ±.
    3. ASLA metni deƒüi≈ütirme, sadece hatalƒ± kelimeyi ve doƒürusunu JSON olarak ver.
    4. Span (ba≈ülangƒ±√ß-biti≈ü) veremiyorsan sadece kelimeyi ver.

    METƒ∞N:
    {full_text}

    √áIKTI FORMATI (JSON):
    {{ "additional_errors": [ {{ "wrong": "...", "correct": "...", "explanation": "..." }} ] }}
    """
    
    llm_errors = []
    # CEFR Puanlarƒ± i√ßin Rubric Prompt
    prompt_rubric = f"""
    ROL: √ñƒüretmen ({data.level}).
    METƒ∞N: \"\"\"{full_text}\"\"\"
    PUANLA (TOPLAM 100):
    Uzunluk(16), Noktalama(14), Dil Bilgisi(16), S√∂z Dizimi(20), Kelime(14), ƒ∞√ßerik(20).
    √áIKTI: {{ "rubric": {{ "uzunluk": 0, "noktalama": 0, "dil_bilgisi": 0, "soz_dizimi": 0, "kelime": 0, "icerik": 0 }}, "teacher_note": "..." }}
    """

    final_result = None

    for model_name in MODELS_TO_TRY:
        try:
            # Paralel √ßaƒürƒ± yerine sƒ±ralƒ± yapƒ±yoruz (g√ºvenlik i√ßin)
            # 1. Hata Tespiti
            resp_err = await asyncio.to_thread(
                client.models.generate_content,
                model=model_name,
                contents=prompt,
                config=types.GenerateContentConfig(response_mime_type="application/json")
            )
            llm_json = safe_json(getattr(resp_err, "text", "") or "")
            raw_llm_errors = llm_json.get("additional_errors", [])

            # 2. Puanlama
            resp_rubric = await asyncio.to_thread(
                client.models.generate_content,
                model=model_name,
                contents=prompt_rubric,
                config=types.GenerateContentConfig(response_mime_type="application/json", temperature=0.1)
            )
            rubric_json = safe_json(getattr(resp_rubric, "text", "") or "")

            # LLM Sonu√ßlarƒ±nƒ± Span ile E≈üle≈ütir (√áakƒ±≈üma Kontrol√º)
            for item in raw_llm_errors:
                wrong_word = item.get("wrong", "")
                if not wrong_word: continue
                
                # Regex ile bu kelimeyi metinde bul
                # Not: Basit search, √ßakƒ±≈üma varsa atla
                match = re.search(re.escape(wrong_word), full_text)
                if match:
                    is_overlap = any(
                        (match.start() < e["span"]["end"] and match.end() > e["span"]["start"])
                        for e in rule_errors
                    )
                    
                    if not is_overlap:
                        llm_errors.append({
                            "wrong": wrong_word,
                            "correct": item.get("correct"),
                            "rule_id": "LLM_SEMANTIC",
                            "span": {"start": match.start(), "end": match.end()},
                            "type": "Kelime Hatasƒ±",
                            "explanation": item.get("explanation"),
                            "confidence": 0.85,
                            "source": "LLM"
                        })

            # Sonu√ßlarƒ± Birle≈ütir
            all_errors = rule_errors + llm_errors
            all_errors.sort(key=lambda x: x["span"]["start"])

            # Puanlama
            rb = rubric_json.get("rubric", {})
            rubric = {
                "uzunluk": to_int(rb.get("uzunluk"), 10),
                "noktalama": to_int(rb.get("noktalama"), 10),
                "dil_bilgisi": to_int(rb.get("dil_bilgisi"), 10),
                "soz_dizimi": to_int(rb.get("soz_dizimi"), 15),
                "kelime": to_int(rb.get("kelime"), 10),
                "icerik": to_int(rb.get("icerik"), 15),
            }
            total_score = sum(rubric.values())

            final_result = {
                "score_total": total_score,
                "rubric": rubric,
                "errors": all_errors,
                "errors_ocr": [], # ≈ûimdilik bo≈ü, istenirse eklenir
                "teacher_note": rubric_json.get("teacher_note", "Analiz tamamlandƒ±."),
                "ai_insight": "Hibrit analiz (Kural + YZ) tamamlandƒ±."
            }
            break # Ba≈üarƒ±lƒ±ysa d√∂ng√ºden √ßƒ±k

        except Exception as e:
            print(f"LLM Hata ({model_name}): {e}")
            continue

    if not final_result:
        raise HTTPException(status_code=500, detail="Analiz ba≈üarƒ±sƒ±z oldu.")

    # Veritabanƒ±na Kayƒ±t
    try:
        supabase.table("submissions").insert({
            "student_name": data.student_name,
            "student_surname": data.student_surname,
            "classroom_code": data.classroom_code,
            "image_url": data.image_url,
            "ocr_text": full_text,
            "level": data.level,
            "country": data.country,
            "native_language": data.native_language,
            "analysis_json": final_result,
            "score_total": final_result["score_total"]
        }).execute()
    except Exception as e:
        print(f"DB Kayƒ±t Hatasƒ±: {e}")

    return {"status": "success", "data": final_result}

@app.post("/student-history")
async def get_student_history(student_name: str = Form(...), student_surname: str = Form(...), classroom_code: str = Form(...)):
    try:
        res = supabase.table("submissions").select("*")\
            .ilike("student_name", student_name.strip())\
            .ilike("student_surname", student_surname.strip())\
            .eq("classroom_code", classroom_code.strip())\
            .order("created_at", desc=True).execute()
        return {"status": "success", "data": res.data}
    except Exception as e: return {"status": "error", "message": str(e)}

@app.post("/update-score")
async def update_score(data: UpdateScoreRequest):
    try:
        res = supabase.table("submissions").select("analysis_json").eq("id", data.submission_id).execute()
        if not res.data: raise HTTPException(status_code=404, detail="Kayƒ±t yok")
        curr = res.data[0]["analysis_json"]
        if "rubric" not in curr: curr["rubric"] = {}
        curr["rubric"].update(data.new_rubric)
        supabase.table("submissions").update({ "score_total": data.new_total, "analysis_json": curr }).eq("id", data.submission_id).execute()
        return {"status": "success", "message": "G√ºncellendi"}
    except Exception as e: raise HTTPException(status_code=500, detail=str(e))