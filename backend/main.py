from fastapi import FastAPI, UploadFile, File, Form, HTTPException
from fastapi.middleware.cors import CORSMiddleware
from google import genai
from google.genai import types
from supabase import create_client, Client
from dotenv import load_dotenv
import os, json, uuid, re
from pydantic import BaseModel
from typing import Union, List, Dict, Any, Optional

# =======================================================
# 1) AYARLAR
# =======================================================
load_dotenv()

API_KEY = os.getenv("GEMINI_API_KEY")
if not API_KEY:
    raise RuntimeError("âŒ KRÄ°TÄ°K HATA: GEMINI_API_KEY eksik!")

SUPABASE_URL = (os.getenv("SUPABASE_URL", "") or "").rstrip("/")
SUPABASE_KEY = os.getenv("SUPABASE_KEY")
if not SUPABASE_URL or not SUPABASE_KEY:
    raise RuntimeError("âŒ KRÄ°TÄ°K HATA: SUPABASE bilgileri eksik!")

client = genai.Client(api_key=API_KEY)
supabase: Client = create_client(SUPABASE_URL, SUPABASE_KEY)

app = FastAPI(title="Sanal Ogretmen AI API", version="1.5.0")

app.add_middleware(
    CORSMiddleware,
    allow_origin_regex=r"https?://(localhost:(3000|5173|8081)|sanal-(ogretmen|ogrenci)-ai(-.*)?\.vercel\.app)",
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# Model fallback
MODELS_TO_TRY = [
    "gemini-2.0-flash-exp",
    "gemini-1.5-flash",
    "gemini-1.5-pro",
]

MAX_FILE_SIZE = 6 * 1024 * 1024
ALLOWED_EXTENSIONS = {"jpg", "jpeg", "png", "webp"}
MIME_BY_EXT = {"jpg": "image/jpeg", "jpeg": "image/jpeg", "png": "image/png", "webp": "image/webp"}

# =======================================================
# 2) HEALTH
# =======================================================
@app.get("/")
@app.get("/health")
def health_check():
    return {"status": "ok", "service": "Sanal Ogretmen AI Backend"}

# =======================================================
# 3) MODELLER
# =======================================================
class AnalyzeRequest(BaseModel):
    ocr_text: str
    image_url: Optional[str] = ""
    student_name: str
    student_surname: str
    classroom_code: str
    level: str
    country: str
    native_language: str

class UpdateScoreRequest(BaseModel):
    submission_id: Union[int, str]
    new_rubric: dict
    new_total: int

# =======================================================
# 4) TDK RULES
# =======================================================
def load_tdk_rules() -> List[Dict[str, Any]]:
    return [
        {"rule_id": "TDK_01_BAGLAC_DE", "text": "BaÄŸlaÃ§ olan 'da/de' ayrÄ± yazÄ±lÄ±r."},
        {"rule_id": "TDK_02_BAGLAC_KI", "text": "BaÄŸlaÃ§ olan 'ki' ayrÄ± yazÄ±lÄ±r."},
        {"rule_id": "TDK_03_SORU_EKI", "text": "Soru eki 'mÄ±/mi' ayrÄ± yazÄ±lÄ±r."},
        {"rule_id": "TDK_04_SEY_SOZ", "text": "'Åey' sÃ¶zcÃ¼ÄŸÃ¼ daima ayrÄ± yazÄ±lÄ±r."},
        {"rule_id": "TDK_05_BUYUK_CUMLE", "text": "CÃ¼mleler bÃ¼yÃ¼k harfle baÅŸlar."},
        {"rule_id": "TDK_06_BUYUK_OZEL", "text": "Ã–zel isimler bÃ¼yÃ¼k harfle baÅŸlar."},
        {"rule_id": "TDK_08_BUYUK_GEREKSIZ", "text": "Ã–zel isim olmayan sÃ¶zcÃ¼kler cÃ¼mle iÃ§inde bÃ¼yÃ¼k harfle yazÄ±lamaz."},
        {"rule_id": "TDK_09_KESME_OZEL", "text": "Ã–zel isimlere gelen ekler kesme ile ayrÄ±lÄ±r (Samsun'a)."},
        {"rule_id": "TDK_13_KESME_GENEL", "text": "Cins isimlere gelen ekler kesme ile ayrÄ±lmaz (stadyuma, okula)."},
        {"rule_id": "TDK_12_SAYILAR", "text": "SayÄ±lar ayrÄ± yazÄ±lÄ±r (on beÅŸ)."},
        {"rule_id": "TDK_20_NOKTA", "text": "CÃ¼mle sonuna nokta konur."},
        {"rule_id": "TDK_21_VIRGUL", "text": "SÄ±ralÄ± kelimelere virgÃ¼l konur."},
        {"rule_id": "TDK_23_YANLIS_YALNIZ", "text": "YanlÄ±ÅŸ (yanÄ±lmak), YalnÄ±z (yalÄ±n)."},
        {"rule_id": "TDK_24_HERKES", "text": "Herkes (s ile)."},
        {"rule_id": "TDK_25_SERTLESME", "text": "SertleÅŸme kuralÄ± (Kitapta, 1923'te)."},
        {"rule_id": "TDK_28_YABANCI", "text": "YabancÄ± kelimeler (ÅofÃ¶r, egzoz, makine)."}
    ]

# =======================================================
# 5) YARDIMCILAR
# =======================================================
_ZERO_WIDTH = re.compile(r"[\u200B\u200C\u200D\uFEFF]")

# âœ… TÃ¼rkÃ§e lower fix
TR_LOWER_MAP = str.maketrans({"Ä°": "i", "I": "Ä±"})
def tr_lower(s: str) -> str:
    if not s:
        return ""
    return s.translate(TR_LOWER_MAP).lower()

def tr_lower_first(word: str) -> str:
    if not word:
        return ""
    return tr_lower(word[0]) + word[1:]

def normalize_text(text: str) -> str:
    """
    âœ… NEWLINE KORUR.
    OCR satÄ±rlarÄ± cÃ¼mle baÅŸÄ± tespiti iÃ§in kritik.
    """
    if not text:
        return ""
    text = text.replace("â€™", "'").replace("`", "'")
    text = _ZERO_WIDTH.sub("", text)

    # Windows newline -> \n
    text = text.replace("\r\n", "\n").replace("\r", "\n")

    # satÄ±r iÃ§i boÅŸluklarÄ± toparla, newline kalsÄ±n
    lines = [re.sub(r"[ \t]+", " ", ln).strip() for ln in text.split("\n")]
    # boÅŸ satÄ±rlarÄ± at
    lines = [ln for ln in lines if ln != ""]
    return "\n".join(lines).strip()

def normalize_match(text: str) -> str:
    return tr_lower(normalize_text(text))

def to_int(x, default=0):
    try:
        if x is None:
            return default
        if isinstance(x, (int, float)):
            return int(x)
        if isinstance(x, str):
            if "/" in x:
                x = x.split("/")[0]
            clean = re.sub(r"[^\d\-]", "", x)
            return int(clean) if clean else default
        return default
    except:
        return default

async def read_limited(upload: UploadFile, limit: int) -> bytes:
    chunks = []
    size = 0
    while True:
        chunk = await upload.read(1024 * 1024)
        if not chunk:
            break
        size += len(chunk)
        if size > limit:
            raise HTTPException(status_code=413, detail=f"Dosya Ã§ok bÃ¼yÃ¼k (Maks {limit // (1024*1024)}MB).")
        chunks.append(chunk)
    return b"".join(chunks)

# CÃ¼mle/segment baÅŸlangÄ±cÄ±: nokta + newline + : ; â€” â€“ --
SENT_BOUNDARY = re.compile(r"([.!?]+|[\n\r]+|[:;]+|â€”|â€“|-{2,})")

def sentence_starts(text: str) -> set:
    starts = {0}
    for m in SENT_BOUNDARY.finditer(text):
        idx = m.end()
        while idx < len(text) and text[idx].isspace():
            idx += 1
        if idx < len(text):
            starts.add(idx)
    return starts

# basit Ã¶zel isim ipuÃ§larÄ± (gerekirse geniÅŸlet)
PROPER_ROOTS = {"samsun", "karadeniz", "tÃ¼rkiye"}

def norm_token(token: str) -> str:
    if not token:
        return ""
    t = token.strip().replace("â€™", "'")
    t = re.sub(r"[.,;:!?()\[\]{}]", "", t)
    return t

def token_root(token: str) -> str:
    t = norm_token(token)
    if "'" in t:
        t = t.split("'")[0]
    return tr_lower(t)

def is_probably_proper(word: str) -> bool:
    r = token_root(word)
    if r in PROPER_ROOTS:
        return True
    if "'" in norm_token(word) and word[:1].isupper():
        return True
    return False

def _find_best_span(full_text: str, wrong: str, hint_start: int = None):
    """
    normalize_match newline korur; ama find iÃ§in line-break farkÄ± sorun olabilir.
    Bu yÃ¼zden aramayÄ± 'display' Ã¼zerinden yapÄ±yoruz: newline -> space
    """
    wrong_n = normalize_match(wrong).replace("\n", " ")
    full_n = normalize_match(full_text).replace("\n", " ")

    if not wrong_n:
        return None

    matches = []
    start_idx = 0
    while True:
        idx = full_n.find(wrong_n, start_idx)
        if idx == -1:
            break
        matches.append(idx)
        start_idx = idx + 1

    if not matches:
        return None

    best = min(matches, key=lambda x: abs(x - hint_start)) if hint_start is not None else matches[0]

    # âš ï¸ burada idx, newline->space dÃ¶nÃ¼ÅŸtÃ¼rÃ¼lmÃ¼ÅŸ metne gÃ¶re
    # Basit yaklaÅŸÄ±m: gerÃ§ek metinde de aynÄ± indeks Ã§oÄŸunlukla tutar (satÄ±r sonu azsa).
    # Daha saÄŸlam istersen mapping yapÄ±lÄ±r ama ÅŸimdilik pratikte yeterli.
    return (best, best + len(wrong_n))

# =======================================================
# 5A) LLM dÃ¼zeltmelerini gÃ¼venli hale getir (paraphrase engeli)
# =======================================================
def is_safe_correction(wrong: str, correct: str) -> bool:
    w = normalize_text(wrong)
    c = normalize_text(correct)
    if not w or not c:
        return False

    # 1) CÃ¼mle/paragraf gibi uzun parÃ§a dÃ¼zeltmesi istemiyoruz
    if len(w) > 25 or "\n" in w:
        return False

    # 2) AÅŸÄ±rÄ± kÄ±saltma (kelime dÃ¼ÅŸÃ¼rme)
    if len(c) < max(2, int(len(w) * 0.75)):
        return False

    # 3) Ã‡ok bÃ¼yÃ¼k deÄŸiÅŸimi engelle (basit karakter set Ã¶rtÃ¼ÅŸmesi)
    w0 = normalize_match(w)
    c0 = normalize_match(c)
    common = set(w0) & set(c0)
    if len(common) / max(1, len(set(w0))) < 0.5:
        return False

    return True

def validate_analysis(result: Dict[str, Any], full_text: str, allowed_ids: set) -> Dict[str, Any]:
    if not isinstance(result, dict):
        return {"errors": []}

    raw_errors = result.get("errors", [])
    if not isinstance(raw_errors, list):
        raw_errors = []

    clean_errors = []
    for err in raw_errors:
        if not isinstance(err, dict):
            continue

        rid = err.get("rule_id")
        if not rid or rid not in allowed_ids:
            continue

        wrong = err.get("wrong", "") or ""
        correct = err.get("correct", "") or ""
        if not wrong or not correct:
            continue
        if normalize_match(wrong) == normalize_match(correct):
            continue

        # âœ… paraphrase/cÃ¼mle bozma filtresi
        if not is_safe_correction(wrong, correct):
            continue

        hint = None
        if isinstance(err.get("span"), dict):
            hint = to_int(err["span"].get("start"), None)

        fixed = _find_best_span(full_text, wrong, hint)
        if fixed:
            start, end = fixed
            # gerÃ§ek metinden slice almak newline index farkÄ±nda sorun Ã§Ä±karabilir.
            # UI iÃ§in wrong olarak LLM wrong'Ä± tutmak daha gÃ¼venli:
            clean_errors.append({
                "wrong": wrong,
                "correct": correct,
                "type": "YazÄ±m",
                "rule_id": rid,
                "explanation": err.get("explanation", ""),
                "span": {"start": start, "end": end},
                "ocr_suspect": bool(err.get("ocr_suspect", False))
            })

    clean_errors.sort(key=lambda x: x["span"]["start"])
    return {"errors": clean_errors}

def merge_and_dedupe_errors(*lists: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
    seen = set()
    merged = []
    for lst in lists:
        for e in (lst or []):
            sp = e.get("span", {}) or {}
            key = (
                sp.get("start"), sp.get("end"),
                normalize_match(e.get("wrong", "")),
                normalize_match(e.get("correct", "")),
                e.get("rule_id")
            )
            if key in seen:
                continue
            seen.add(key)
            merged.append(e)
    merged.sort(key=lambda x: x.get("span", {}).get("start", 10**9))
    return merged

# =======================================================
# 5B) OCR ÅÃœPHELÄ° TESPÄ°T
# =======================================================
OCR_NOISE_PATTERNS = [
    re.compile(r".*\b[a-zA-ZÄŸÃ¼ÅŸÃ¶Ã§Ä±Ä°ÄÃœÅÃ–Ã‡]+['â€™][a-zA-Z]\b"),
    re.compile(r"^[a-zA-Z]\b"),
]

def looks_like_ocr_noise(wrong: str, full_text: str, span: dict) -> bool:
    w = (wrong or "").strip()
    if len(w) <= 1:
        return True
    for p in OCR_NOISE_PATTERNS:
        if p.search(w):
            if " " in w and len(w.split()) == 2 and len(w.split()[1]) == 1:
                return True
    return False

# =======================================================
# 5C) BÃ¼yÃ¼k harf (false-positive azaltÄ±lmÄ±ÅŸ)
# =======================================================
def find_unnecessary_capitals(full_text: str) -> list:
    starts = sentence_starts(full_text)
    errors = []

    for m in re.finditer(r"\b[^\W\d_]+\b", full_text, flags=re.UNICODE):
        word = m.group(0)
        s, e = m.start(), m.end()

        if s in starts:
            continue

        if is_probably_proper(word):
            continue

        # "Sok" kelimesi Ã§oÄŸu zaman OCR->Ã§ok; bÃ¼yÃ¼k harf Ã¶nerisi Ã¼retme
        if tr_lower(word) in {"sok"}:
            continue

        upp = sum(1 for ch in word if ch.isupper())
        low = sum(1 for ch in word if ch.islower())
        is_weird_case = (upp >= 2 and low >= 1)

        if is_weird_case:
            errors.append({
                "wrong": word,
                "correct": word,
                "type": "OCR_ÅÃœPHELÄ°",
                "rule_id": "TDK_08_BUYUK_GEREKSIZ",
                "explanation": "BÃ¼yÃ¼k/kÃ¼Ã§Ã¼k harf karÄ±ÅŸÄ±klÄ±ÄŸÄ± OCR kaynaklÄ± olabilir.",
                "span": {"start": s, "end": e},
                "ocr_suspect": True
            })
            continue

        if word and word[0].isupper():
            errors.append({
                "wrong": word,
                "correct": tr_lower_first(word),
                "type": "BÃ¼yÃ¼k Harf",
                "rule_id": "TDK_08_BUYUK_GEREKSIZ",
                "explanation": "CÃ¼mle ortasÄ±nda gereksiz bÃ¼yÃ¼k harf kullanÄ±mÄ±.",
                "span": {"start": s, "end": e},
                "ocr_suspect": False
            })

    return errors

# =======================================================
# 5D) A2 heuristics
# =======================================================
POSSESSIVE_HINT = re.compile(r"(Ä±m|im|um|Ã¼m|Ä±n|in|un|Ã¼n|m|n)$", re.IGNORECASE | re.UNICODE)

def find_conjunction_dade_joined(full_text: str) -> list:
    errs = []
    for m in re.finditer(r"\b([^\W\d_]+)(da|de)\b", full_text, flags=re.UNICODE | re.IGNORECASE):
        base = m.group(1)
        suf = m.group(2)
        whole = full_text[m.start():m.end()]

        # mektubun-da, evim-de gibi -> bÃ¶lme
        if POSSESSIVE_HINT.search(base):
            continue

        # Ã¶zel isim/bÃ¼yÃ¼k harf varsa dokunma
        if any(ch.isupper() for ch in whole) or is_probably_proper(whole):
            continue

        errs.append({
            "wrong": whole,
            "correct": f"{base} {suf}",
            "type": "YazÄ±m",
            "rule_id": "TDK_01_BAGLAC_DE",
            "explanation": "BaÄŸlaÃ§ olan da/de ayrÄ± yazÄ±lÄ±r. (DÃ¼ÅŸÃ¼k gÃ¼ven: ek de olabilir.)",
            "span": {"start": m.start(), "end": m.end()},
            "ocr_suspect": True
        })
    return errs

def find_common_a2_errors(full_text: str) -> list:
    errs = []

    # cok/Ã§ok OCR varyantlarÄ±
    for m in re.finditer(r"\b(cok|Ã§og|cÃ¶k|coK|COk|sok)\b", full_text, flags=re.IGNORECASE):
        wrong = m.group(0)
        errs.append({
            "wrong": wrong,
            "correct": "Ã§ok",
            "type": "YazÄ±m",
            "rule_id": "TDK_28_YABANCI",
            "explanation": "â€˜Ã§okâ€™ kelimesinin yazÄ±mÄ±.",
            "span": {"start": m.start(), "end": m.end()},
            "ocr_suspect": True
        })

    # soru eki bitiÅŸik: geldinmi -> geldin mi
    for m in re.finditer(r"\b([^\W\d_]{2,})(mi|mÄ±|mu|mÃ¼)\b", full_text, flags=re.UNICODE | re.IGNORECASE):
        word = m.group(0)
        wl = tr_lower(word)
        if wl in {"kimi", "bimi"}:
            continue
        errs.append({
            "wrong": word,
            "correct": m.group(1) + " " + m.group(2),
            "type": "YazÄ±m",
            "rule_id": "TDK_03_SORU_EKI",
            "explanation": "Soru eki ayrÄ± yazÄ±lÄ±r.",
            "span": {"start": m.start(), "end": m.end()},
            "ocr_suspect": False
        })

    return errs

# =======================================================
# 5E) Tek span tek Ã¶neri
# =======================================================
RULE_PRIORITY = {
    "TDK_28_YABANCI": 100,
    "TDK_03_SORU_EKI": 90,
    "TDK_09_KESME_OZEL": 80,
    "TDK_13_KESME_GENEL": 80,
    "TDK_25_SERTLESME": 70,
    "TDK_01_BAGLAC_DE": 60,
    "TDK_08_BUYUK_GEREKSIZ": 30,
    "TDK_05_BUYUK_CUMLE": 20,
    "TDK_20_NOKTA": 20,
    "TDK_21_VIRGUL": 20,
    "TDK_23_YANLIS_YALNIZ": 10,
}

def pick_best_per_span(errors: list) -> list:
    buckets: Dict[tuple, List[dict]] = {}
    for e in errors:
        sp = e.get("span") or {}
        key = (sp.get("start"), sp.get("end"))
        if None in key:
            continue
        buckets.setdefault(key, []).append(e)

    chosen = []
    for _, items in buckets.items():
        def score(e):
            pri = RULE_PRIORITY.get(e.get("rule_id"), 0)
            ocr_penalty = 20 if e.get("ocr_suspect") else 0
            same_penalty = 50 if normalize_match(e.get("wrong","")) == normalize_match(e.get("correct","")) else 0
            return pri - ocr_penalty - same_penalty
        chosen.append(max(items, key=score))

    chosen.sort(key=lambda x: x["span"]["start"])
    return chosen

# =======================================================
# 5F) CEFR fallback (0 puanlarÄ± bitirmek iÃ§in)
# =======================================================
def cefr_fallback_scores(level: str, text: str) -> Dict[str, int]:
    t = normalize_text(text).replace("\n", " ")
    if not t:
        return {"uzunluk": 0, "soz_dizimi": 0, "kelime": 0, "icerik": 0}

    words = re.findall(r"\b[^\W\d_]+\b", t, flags=re.UNICODE)
    sentences = [s for s in re.split(r"[.!?]+", t) if s.strip()]
    has_connectors = bool(re.search(r"\b(ve|ama|Ã§Ã¼nkÃ¼|bu yÃ¼zden|sonra|fakat)\b", tr_lower(t)))
    uniq = len(set([tr_lower(w) for w in words])) if words else 0

    uzunluk = min(16, max(4, int(len(words) / 10) + 6))
    kelime = min(14, max(5, int(uniq / 8) + 6))

    soz = 8
    if len(sentences) >= 3:
        soz += 4
    if has_connectors:
        soz += 4
    soz_dizimi = min(20, max(6, soz))

    icerik = 8
    if len(sentences) >= 3:
        icerik += 4
    if len(words) >= 40:
        icerik += 4
    icerik = min(20, max(6, icerik))

    return {"uzunluk": int(uzunluk), "soz_dizimi": int(soz_dizimi), "kelime": int(kelime), "icerik": int(icerik)}

# =======================================================
# 6) ENDPOINTS
# =======================================================
@app.get("/check-class/{code}")
async def check_class_code(code: str):
    try:
        response = supabase.table("classrooms").select("name").eq("code", code.upper().strip()).execute()
        if response.data:
            return {"valid": True, "class_name": response.data[0]["name"]}
        return {"valid": False}
    except:
        return {"valid": False}

@app.post("/ocr")
async def ocr_image(file: UploadFile = File(...), classroom_code: str = Form(...)):
    try:
        file_content = await read_limited(file, MAX_FILE_SIZE)

        filename = file.filename or "unknown.jpg"
        file_ext = "jpg"
        if "." in filename:
            ext = filename.rsplit(".", 1)[-1].lower()
            if ext in ALLOWED_EXTENSIONS:
                file_ext = ext

        safe_mime = file.content_type
        if not safe_mime or not safe_mime.startswith("image/"):
            safe_mime = MIME_BY_EXT.get(file_ext, "image/jpeg")

        safe_code = re.sub(r"[^A-Za-z0-9_-]", "_", classroom_code)[:20]
        unique_filename = f"{safe_code}_{uuid.uuid4()}.{file_ext}"
        image_url = ""

        try:
            supabase.storage.from_("odevler").upload(
                unique_filename, file_content, {"content-type": safe_mime, "upsert": "false"}
            )
            res = supabase.storage.from_("odevler").get_public_url(unique_filename)
            image_url = res if isinstance(res, str) else res.get("publicUrl")
        except Exception as up_err:
            print(f"âš ï¸ Upload UyarÄ±sÄ±: {up_err}")

        # =======================================================
        # 1) AÅAMA: HAM OCR (DÃœZELTME YOK)
        # =======================================================
        extracted_text = ""
        prompt_ocr = """ROL: Sen bir OCR katibisin. Ã–ÄŸretmen deÄŸilsin. Dil uzmanÄ± deÄŸilsin.

GÃ–REV:
Bu gÃ¶rseldeki EL YAZISI TÃ¼rkÃ§e metni, KAÄITTA GÃ–RDÃœÄÃœN GÄ°BÄ° dijital metne aktar.

KESÄ°N KURALLAR:
- ASLA dÃ¼zeltme yapma.
- ASLA kelimeyi daha doÄŸru / daha anlamlÄ± hale getirme.
- ASLA yazÄ±m, noktalama, bÃ¼yÃ¼k/kÃ¼Ã§Ã¼k harf, ek dÃ¼zeltmesi yapma.
- YanlÄ±ÅŸ olduÄŸunu dÃ¼ÅŸÃ¼nsen bile, gÃ¶rdÃ¼ÄŸÃ¼nÃ¼ yaz.

BÄ°Ã‡Ä°M:
- SATIRLARI KORU (kaÄŸÄ±ttaki satÄ±r sonlarÄ± kalsÄ±n).
- Yorum/baÅŸlÄ±k/aÃ§Ä±klama ekleme.
- SADECE metni yaz.

Ã‡IKTI:
SADECE OCR METNÄ°. BAÅKA HÄ°Ã‡BÄ°R ÅEY YAZMA.
"""

        for model_name in MODELS_TO_TRY:
            try:
                resp = client.models.generate_content(
                    model=model_name,
                    contents=[prompt_ocr, types.Part.from_bytes(data=file_content, mime_type=safe_mime)],
                    config=types.GenerateContentConfig(
                        temperature=0,
                        response_mime_type="text/plain",
                    ),
                )
                extracted_text = (resp.text or "").strip().replace("```", "").strip()
                if extracted_text:
                    break
            except Exception:
                continue

        if not extracted_text:
            return {"status": "error", "message": "OCR BaÅŸarÄ±sÄ±z"}

        raw_text = extracted_text

        # =======================================================
        # 2) AÅAMA: EMNÄ°YET FÄ°LTRESÄ° (DÃœZELTME YOK, SADECE â° Ä°ÅARETLE)
        #    AmaÃ§: LLM tahmin edip "emin deÄŸilim" demese bile,
        #    TÃ¼rkÃ§e-dÄ±ÅŸÄ± / ÅŸÃ¼pheli kelimelerde karakter maskele.
        # =======================================================
        MARK_CHAR = "â°"
        WORD_MARK = "(â°)"

        VOWELS = set("aeÄ±ioÃ¶uÃ¼AEIÄ°OÃ–UÃœ")
        # TÃ¼rkÃ§e'de Ã§ok nadir/ÅŸÃ¼pheli gÃ¶rÃ¼len bazÄ± ikililer (heuristic)
        SUSPICIOUS_BIGRAMS = {
            "tz", "zd", "dt", "tb", "pk", "bp", "gk", "kg", "qy", "wq", "xq",
            "yiÌ‡",  # bazÄ± OCR birleÅŸimleri
        }
        # TÃ¼rkÃ§e'de beklenmeyen karakterler (rakam, ascii dÄ±ÅŸÄ± vb.)
        ALLOWED_CHARS = set("abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZÃ§Ã‡ÄŸÄÄ±Ä°Ã¶Ã–ÅŸÅÃ¼Ãœ'-.,;:!?()â€œâ€\" \n\t")

        def is_turkish_like_word(w: str) -> bool:
            # Sadece harflerden oluÅŸmayan parÃ§alarÄ± (Ã¶r. "2026", "â€”") kelime saymayalÄ±m
            letters = [c for c in w if c.isalpha() or c in "Ã§Ã‡ÄŸÄÄ±Ä°Ã¶Ã–ÅŸÅÃ¼Ãœ"]
            if not letters:
                return True

            # En az 1 Ã¼nlÃ¼ yoksa Ã§ok ÅŸÃ¼pheli (Ã¶rn: "trl", "stdy")
            if not any(c in VOWELS for c in letters):
                return False

            # 4+ Ã¼nsÃ¼z ardÄ±ÅŸÄ±k ise ÅŸÃ¼pheli
            consec_cons = 0
            for c in letters:
                if c in VOWELS:
                    consec_cons = 0
                else:
                    consec_cons += 1
                    if consec_cons >= 4:
                        return False

            # Basit bigram kontrolÃ¼
            low = "".join(letters).lower()
            for i in range(len(low) - 1):
                bg = low[i:i+2]
                if bg in SUSPICIOUS_BIGRAMS:
                    return False

            return True

        def mask_one_char(word: str) -> str:
            """
            Kelimeyi ASLA dÃ¼zeltmez.
            Sadece ÅŸÃ¼pheli kelimede 1 karakteri â° yapar.
            Hangi karakter?:
              - Harf olmayan/garip karakter varsa onu maskeler.
              - Yoksa uzun Ã¼nsÃ¼z dizisinin ortasÄ±nÄ± maskeler.
              - HÃ¢lÃ¢ seÃ§emezse sonuna (â°) ekler.
            """
            # Garip karakter yakala
            for idx, ch in enumerate(word):
                if ch not in ALLOWED_CHARS:
                    return word[:idx] + MARK_CHAR + word[idx+1:]

            # Harf dizisi Ã¼zerinden Ã¼nsÃ¼z serisi bul
            letters_idx = [(i, ch) for i, ch in enumerate(word) if ch.isalpha() or ch in "Ã§Ã‡ÄŸÄÄ±Ä°Ã¶Ã–ÅŸÅÃ¼Ãœ"]
            if not letters_idx:
                return word + WORD_MARK

            # 4+ Ã¼nsÃ¼z koÅŸusunu bul
            run = []
            current = []
            for i, ch in letters_idx:
                if ch in VOWELS:
                    if len(current) >= 4:
                        run = current
                        break
                    current = []
                else:
                    current.append((i, ch))
            if not run and len(current) >= 4:
                run = current

            if run:
                mid_i = run[len(run)//2][0]
                return word[:mid_i] + MARK_CHAR + word[mid_i+1:]

            # Bigram yakala ve ikinci harfi maskele
            low = "".join(ch for _, ch in letters_idx).lower()
            if len(low) >= 2:
                for j in range(len(low) - 1):
                    if low[j:j+2] in SUSPICIOUS_BIGRAMS:
                        # j+1'in orijinal indexini bul
                        orig_i = letters_idx[j+1][0]
                        return word[:orig_i] + MARK_CHAR + word[orig_i+1:]

            # Son Ã§are: kelime sonuna iÅŸaret
            return word + WORD_MARK

        def post_mask_text(text: str) -> str:
            # SatÄ±rlarÄ± koruyarak token bazlÄ± ilerleyelim.
            # BoÅŸluklarÄ± kaybetmemek iÃ§in split deÄŸil, regex ile token yakalayacaÄŸÄ±z.
            def repl(m):
                token = m.group(0)
                # Ã‡ok kÄ±sa ÅŸeylere dokunma
                if len(token) <= 2:
                    return token
                # Kelime benzeri token ise kontrol et
                if any(ch.isalpha() or ch in "Ã§Ã‡ÄŸÄÄ±Ä°Ã¶Ã–ÅŸÅÃ¼Ãœ" for ch in token):
                    if not is_turkish_like_word(token):
                        return mask_one_char(token)
                return token

            # "kelime benzeri" parÃ§alarÄ± yakala (boÅŸluk/enter aynen kalsÄ±n)
            # Not: apostrof ve tireyi de token iÃ§ine alÄ±yoruz.
            pattern = r"[A-Za-zÃ‡ÄÄ°Ã–ÅÃœÃ§ÄŸÄ±Ã¶ÅŸÃ¼'â€™-]+"
            return re.sub(pattern, repl, text)

        flagged_text = post_mask_text(raw_text)

        return {
            "status": "success",
            "ocr_text": flagged_text,      # Ã¶ÄŸrenciye gÃ¶ster (â° iÅŸaretli)
            "raw_ocr_text": raw_text,      # opsiyonel debug
            "image_url": image_url,

            # UX metinleri
            "ocr_notice": f"â„¹ï¸ Turuncu iÅŸaretli ({MARK_CHAR}) yerler OCR tarafÄ±ndan net okunamamÄ±ÅŸtÄ±r. LÃ¼tfen metni kontrol edip dÃ¼zeltiniz.",
            "ocr_hover_text": "OCR bu kÄ±smÄ± net okuyamadÄ±. Ã–ÄŸrenci kontrol etmelidir.",
            "ocr_markers": { "char": MARK_CHAR, "word": WORD_MARK }
        }

    except HTTPException:
        raise
    except Exception as e:
        return {"status": "error", "message": str(e)}



@app.post("/analyze")
async def analyze_submission(data: AnalyzeRequest):
    if not data.ocr_text or not data.ocr_text.strip():
        raise HTTPException(status_code=400, detail="Metin boÅŸ, analiz yapÄ±lamaz.")

    full_text = normalize_text(data.ocr_text)  # âœ… newline korunuyor
    display_text = full_text.replace("\n", " ")  # LLM iÃ§in

    print(f"ğŸ§  Analiz: {data.student_name} ({data.level})")

    tdk_rules = load_tdk_rules()
    allowed_ids = {r["rule_id"] for r in tdk_rules}
    rules_text = "\n".join([f"- {r['rule_id']}: {r['text']}" for r in tdk_rules])

    # =======================================================
    # 1) TDK AGENT (paraphrase yasak)
    # =======================================================
    prompt_tdk = f"""
ROL: Sen nesnel ve kuralcÄ± bir TDK denetÃ§isisin.
GÃ–REV: Metindeki yazÄ±m / noktalama / bÃ¼yÃ¼k-kÃ¼Ã§Ã¼k harf / kesme iÅŸareti / ek yazÄ±mÄ± hatalarÄ±nÄ± bul.

Ã–NEMLÄ°:
- YENÄ°DEN YAZIM YAPMA: CÃ¼mleleri daha doÄŸal hale getirip yeniden yazma.
- SADECE LOKAL DÃœZELT: 1-2 kelimelik kÃ¼Ã§Ã¼k parÃ§alar dÃ¼zelt.
- wrong alanÄ± en fazla 25 karakterlik kÃ¼Ã§Ã¼k bir parÃ§a olsun. (CÃ¼mle komple olmaz.)
- "wrong" alanÄ±na metindeki parÃ§ayÄ± BÄ°REBÄ°R yaz.
- OCR kaynaklÄ± olabilecek parÃ§alanmalarÄ± "ocr_suspect": true olarak iÅŸaretle.
- Cins isimlerde kesme kullanÄ±lmaz (stadyuma). Ã–zel isimlerde kesme olabilir (Samsun'a).

METÄ°N: \"\"\"{display_text}\"\"\"

REFERANS KURALLAR:
{rules_text}

Ã‡IKTI (SADECE JSON):
{{
  "rubric_part": {{ "noktalama": (0-14 Int), "dil_bilgisi": (0-16 Int) }},
  "errors": [
    {{
      "wrong": "...",
      "correct": "...",
      "rule_id": "...",
      "explanation": "...",
      "span": {{ "start": 0 }},
      "ocr_suspect": false
    }}
  ]
}}
"""

    # =======================================================
    # 2) CEFR AGENT
    # =======================================================
    prompt_cefr = f"""
ROL: Sen destekleyici bir Ã¶ÄŸretmensin.
GÃ–REV: {data.level} seviyesindeki Ã¶ÄŸrencinin iletiÅŸim becerisini deÄŸerlendir.

KURALLAR:
1) YazÄ±m/noktalama hatalarÄ±nÄ± puanlamada ikinci plana at (iletiÅŸim Ã¶ncelikli).
2) PUANLAMA: Tam sayÄ±.
3) teacher_note baÅŸÄ±na "[SEVÄ°YE: ...]" ekle.
4) rubric_part iÃ§indeki 4 alan ZORUNLU ve Int olmalÄ±.

METÄ°N: \"\"\"{display_text}\"\"\"

Ã‡IKTI (SADECE JSON):
{{
  "rubric_part": {{
    "uzunluk": (0-16 Int),
    "soz_dizimi": (0-20 Int),
    "kelime": (0-14 Int),
    "icerik": (0-20 Int)
  }},
  "teacher_note": "[SEVÄ°YE: UYGUN] ..."
}}
"""

    final_result = None
    last_error = ""

    for model_name in MODELS_TO_TRY:
        try:
            print(f"ğŸ”„ Model: {model_name}")

            resp_tdk = client.models.generate_content(
                model=model_name,
                contents=prompt_tdk,
                config=types.GenerateContentConfig(
                    response_mime_type="application/json",
                    temperature=0
                ),
            )
            raw_tdk = (resp_tdk.text or "").strip()
            if not raw_tdk:
                raise ValueError("BoÅŸ TDK YanÄ±tÄ±")
            json_tdk = json.loads(raw_tdk.replace("```json", "").replace("```", ""))

            resp_cefr = client.models.generate_content(
                model=model_name,
                contents=prompt_cefr,
                config=types.GenerateContentConfig(
                    response_mime_type="application/json",
                    temperature=0
                ),
            )
            raw_cefr = (resp_cefr.text or "").strip()
            if not raw_cefr:
                raise ValueError("BoÅŸ CEFR YanÄ±tÄ±")
            json_cefr = json.loads(raw_cefr.replace("```json", "").replace("```", ""))

            # =======================================================
            # PUAN BÄ°RLEÅTÄ°RME + FALLBACK
            # =======================================================
            tdk_p = json_tdk.get("rubric_part", {}) if isinstance(json_tdk, dict) else {}
            cefr_p = json_cefr.get("rubric_part", {}) if isinstance(json_cefr, dict) else {}

            if not isinstance(cefr_p, dict) or not cefr_p:
                cefr_p = cefr_fallback_scores(data.level, full_text)

            combined_rubric = {
                "noktalama": min(14, max(0, to_int(tdk_p.get("noktalama")))),
                "dil_bilgisi": min(16, max(0, to_int(tdk_p.get("dil_bilgisi")))),
                "uzunluk": min(16, max(0, to_int(cefr_p.get("uzunluk")))),
                "soz_dizimi": min(20, max(0, to_int(cefr_p.get("soz_dizimi")))),
                "kelime": min(14, max(0, to_int(cefr_p.get("kelime")))),
                "icerik": min(20, max(0, to_int(cefr_p.get("icerik")))),
            }
            total_score = sum(combined_rubric.values())

            # =======================================================
            # HATA TOPLAMA
            # =======================================================
            cleaned_tdk = validate_analysis(json_tdk, full_text, allowed_ids)
            rule_caps = find_unnecessary_capitals(full_text)
            rule_common = find_common_a2_errors(full_text)
            rule_dade = find_conjunction_dade_joined(full_text)

            all_errors = merge_and_dedupe_errors(
                cleaned_tdk.get("errors", []),
                rule_caps,
                rule_common,
                rule_dade
            )

            # âœ… aynÄ± span Ã§atÄ±ÅŸmasÄ±nÄ± tek Ã¶neriye indir
            all_errors = pick_best_per_span(all_errors)

            # =======================================================
            # OCR / Ã–ÄRENCÄ° AYIR
            # =======================================================
            errors_student = []
            errors_ocr = []

            for e in all_errors:
                span = e.get("span") or {}
                if "start" not in span or "end" not in span:
                    continue

                ocr_flag = bool(e.get("ocr_suspect", False)) or looks_like_ocr_noise(e.get("wrong", ""), full_text, span)
                if ocr_flag:
                    e["type"] = "OCR_ÅÃœPHELÄ°"
                    e["explanation"] = (e.get("explanation", "") + " (OCR parÃ§alanmasÄ± olabilir.)").strip()
                    e["ocr_suspect"] = True
                    errors_ocr.append(e)
                else:
                    errors_student.append(e)

            errors_student.sort(key=lambda x: x["span"]["start"])
            errors_ocr.sort(key=lambda x: x["span"]["start"])

            raw_note = (json_cefr.get("teacher_note") or "").strip() if isinstance(json_cefr, dict) else ""
            if not raw_note:
                raw_note = f"[SEVÄ°YE: {data.level}] DeÄŸerlendirme notu oluÅŸturulamadÄ±."
            elif not raw_note.startswith("["):
                raw_note = f"[SEVÄ°YE: {data.level}] " + raw_note

            final_result = {
                "rubric": combined_rubric,

                # eski UI uyumluluÄŸu: errors = Ã¶ÄŸrenci hatalarÄ±
                "errors": errors_student,

                "errors_student": errors_student,
                "errors_ocr": errors_ocr,

                "teacher_note": raw_note,
                "score_total": total_score
            }

            print(f"âœ… BaÅŸarÄ±lÄ±: {model_name} | Puan: {total_score} | Ã–ÄŸrenci: {len(errors_student)} | OCR: {len(errors_ocr)}")
            break

        except Exception as e:
            print(f"âš ï¸ Hata ({model_name}): {e}")
            last_error = str(e)
            continue

    if not final_result:
        raise HTTPException(status_code=500, detail=f"Analiz baÅŸarÄ±sÄ±z: {last_error}")

    # DB kayÄ±t
    try:
        supabase.table("submissions").insert({
            "student_name": data.student_name.strip(),
            "student_surname": data.student_surname.strip(),
            "classroom_code": data.classroom_code.strip(),
            "image_url": data.image_url,
            "ocr_text": full_text,  # âœ… newline ile saklanÄ±r
            "level": data.level,
            "country": data.country,
            "native_language": data.native_language,
            "analysis_json": final_result,
            "score_total": final_result["score_total"]
        }).execute()

        return {"status": "success", "data": final_result}

    except Exception as e:
        print(f"DB KayÄ±t HatasÄ±: {e}")
        return {"status": "success", "data": final_result, "warning": "VeritabanÄ± hatasÄ±"}

@app.post("/student-history")
async def get_student_history(student_name: str = Form(...), student_surname: str = Form(...), classroom_code: str = Form(...)):
    try:
        response = supabase.table("submissions").select("*")\
            .ilike("student_name", student_name.strip())\
            .ilike("student_surname", student_surname.strip())\
            .eq("classroom_code", classroom_code.strip())\
            .order("created_at", desc=True).execute()
        return {"status": "success", "data": response.data}
    except Exception as e:
        return {"status": "error", "message": str(e)}

@app.post("/update-score")
async def update_score(data: UpdateScoreRequest):
    try:
        res = supabase.table("submissions").select("analysis_json").eq("id", data.submission_id).execute()
        if not res.data:
            raise HTTPException(status_code=404, detail="KayÄ±t bulunamadÄ±")

        current_json = res.data[0].get("analysis_json") or {}
        if "rubric" not in current_json:
            current_json["rubric"] = {}
        current_json["rubric"].update(data.new_rubric)

        supabase.table("submissions").update({
            "score_total": data.new_total,
            "analysis_json": current_json
        }).eq("id", data.submission_id).execute()

        return {"status": "success", "message": "Puan gÃ¼venle gÃ¼ncellendi"}

    except HTTPException:
        raise
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))
