from fastapi import FastAPI, UploadFile, File, Form, HTTPException
from fastapi.middleware.cors import CORSMiddleware
from google import genai
from google.genai import types
from google.cloud import vision
from supabase import create_client, Client
from dotenv import load_dotenv
import os, json, uuid, re
import unicodedata
from pydantic import BaseModel
from typing import Union, List, Dict, Any, Optional
import asyncio

# =======================================================
# 1) AYARLAR VE KURULUM
# =======================================================
load_dotenv()

API_KEY = os.getenv("GEMINI_API_KEY")
if not API_KEY:
    raise RuntimeError("‚ùå KRƒ∞Tƒ∞K HATA: GEMINI_API_KEY eksik!")

SUPABASE_URL = (os.getenv("SUPABASE_URL", "") or "").rstrip("/")
SUPABASE_KEY = os.getenv("SUPABASE_KEY")
if not SUPABASE_URL or not SUPABASE_KEY:
    raise RuntimeError("‚ùå KRƒ∞Tƒ∞K HATA: SUPABASE bilgileri eksik!")

client = genai.Client(api_key=API_KEY)
supabase: Client = create_client(SUPABASE_URL, SUPABASE_KEY)

app = FastAPI(title="Sanal Ogretmen AI API - TUBITAK Hybrid Edition", version="5.1.0")

app.add_middleware(
    CORSMiddleware,
    allow_origin_regex=r"https?://(localhost:(3000|5173|8081)|sanal-(ogretmen|ogrenci)-ai(-.*)?\.vercel\.app)",
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

MODELS_TO_TRY = ["gemini-2.0-flash", "gemini-1.5-flash"]
MAX_FILE_SIZE = 6 * 1024 * 1024

# =======================================================
# 2) AKADEMƒ∞K REFERANS VERƒ∞ SETLERƒ∞ VE REGEX (DESENLER)
# =======================================================

# Yanlƒ±≈ü pozitifleri engellemek i√ßin istisna listeleri
MI_SUFFIX_BLACKLIST = {
    "cami", "mami", "hami", "samimi", "kimi", "t√ºm√º", "ilhami", "resmi", "cismi",
    "ismi", "yemi", "gemisi", "sevgilisi", "kendisi", "annesi", "babasƒ±", "abisi",
    "mermi", "irmi", "vermi", "gemi", "komi"
}

# √ñzel ƒ∞simler (Kesme ile ayrƒ±lmasƒ± gerekenler / B√∂l√ºnmemesi gerekenler)
PROPER_NOUNS_WHITELIST = {
    "t√ºrkiye", "samsun", "istanbul", "ankara", "izmir", "atat√ºrk", "mehmet", "ahmet",
    "ay≈üe", "fatma", "ali", "veli", "atakum", "ilkadƒ±m", "canik", "√ßar≈üamba", "bafra",
    "ingilizce", "t√ºrk√ße", "almanca", "fransƒ±zca", "allah", "tanrƒ±", "mardin", "mersin"
}

# Cins ƒ∞simler (B√ºy√ºk harfle yazƒ±ldƒ±ysa k√º√ß√ºlt√ºlmesi gerekenler)
COMMON_NOUNS = {
    "okul", "kitap", "kalem", "masa", "sandalye", "araba", "ev", "bah√ße", "≈üehir", 
    "insan", "√ßocuk", "kadƒ±n", "adam", "sokak", "mahalle", "k√∂y", "su", "ekmek", 
    "√ßay", "kahve", "√ßok", "pek", "g√ºzel", "iyi", "k√∂t√º", "b√ºy√ºk", "k√º√ß√ºk"
}

# Regex Kalƒ±plarƒ±
PATTERNS = {
    "TDK_03_SORU_EKI": re.compile(r"\b(\w{2,})(mi|mƒ±|mu|m√º)(?=[?.!,;:\s]|$)", re.IGNORECASE | re.UNICODE),
    "TDK_04_SEY_AYRI": re.compile(r"\b(\w+)≈üey\b", re.IGNORECASE | re.UNICODE),
    "TDK_06_YA_DA": re.compile(r"\byada\b", re.IGNORECASE | re.UNICODE),
    "TDK_07_HER_SEY": re.compile(r"\bher≈üey\b", re.IGNORECASE | re.UNICODE),
    "TDK_44_BIRKAC": re.compile(r"\bbir\s+ka√ß\b", re.IGNORECASE | re.UNICODE),
    "TDK_45_HICBIR": re.compile(r"\bhi√ß\s+bir\b", re.IGNORECASE | re.UNICODE),
    "TDK_46_PEKCOK": re.compile(r"\bpek√ßok\b", re.IGNORECASE | re.UNICODE),
    "TDK_41_HERKES": re.compile(r"\bherkez\b", re.IGNORECASE | re.UNICODE),
    "TDK_42_YALNIZ": re.compile(r"\byanliz\b", re.IGNORECASE | re.UNICODE),
    "TDK_43_YANLIS": re.compile(r"\byanlis\b", re.IGNORECASE | re.UNICODE),
    "TDK_47_INSALLAH": re.compile(r"\binsallah\b", re.IGNORECASE | re.UNICODE),
    "TDK_23_KESME_GENEL": re.compile(r"\b([a-z√ßƒüƒ±√∂≈ü√º]{3,})'([a-z√ßƒüƒ±√∂≈ü√º]+)\b", re.UNICODE)
}

# √ñzel ƒ∞sim Soneki Yakalayƒ±cƒ± (B√ºy√ºk harfle ba≈ülayan kelime + ek)
PROPER_NOUN_SUFFIX_REGEX = re.compile(
    r"\b([A-Z√áƒûƒ∞√ñ≈û√ú][a-z√ßƒüƒ±√∂≈ü√º]{2,})(nin|nƒ±n|nun|n√ºn|in|ƒ±n|un|√ºn|de|da|den|dan|e|a|i|ƒ±|u|√º|le|la)\b",
    re.UNICODE
)

# Gereksiz B√ºy√ºk Harf Yakalayƒ±cƒ±
CAPITALIZED_WORD_REGEX = re.compile(r"\b[A-Z√áƒûƒ∞√ñ≈û√ú][a-z√ßƒüƒ±√∂≈ü√º]+\b", re.UNICODE)

# =======================================================
# 3) DATA MODELS & HELPERS
# =======================================================
class AnalyzeRequest(BaseModel):
    ocr_text: str
    image_url: Optional[str] = ""
    student_name: str
    student_surname: str
    classroom_code: str
    level: str
    country: str
    native_language: str

class UpdateScoreRequest(BaseModel):
    submission_id: Union[int, str]
    new_rubric: dict
    new_total: int

def normalize_text(text: str) -> str:
    if not text: return ""
    text = text.replace("‚Äô", "'").replace("`", "'")
    text = unicodedata.normalize("NFKC", text)
    return text.strip()

def tr_lower(text: str) -> str:
    return text.replace("ƒ∞", "i").replace("I", "ƒ±").lower()

def safe_json(text: str) -> dict:
    if not text: return {}
    t = text.strip().replace("```json", "").replace("```", "").strip()
    try: return json.loads(t)
    except: return {}

def to_int(x, default=0):
    try:
        if x is None: return default
        if isinstance(x, (int, float)): return int(x)
        if isinstance(x, str):
            clean = re.sub(r"[^\d\-]", "", x.split("/")[0])
            return int(clean) if clean else default
        return default
    except: return default

def get_sentence_starts(text: str) -> set:
    starts = {0}
    for match in re.finditer(r"[.!?]\s+", text):
        starts.add(match.end())
    return starts

async def ensure_gcp_credentials():
    if os.getenv("GOOGLE_APPLICATION_CREDENTIALS"): return
    key_json = os.getenv("GCP_SA_KEY_JSON", "").strip()
    if not key_json: return
    try:
        path = "/tmp/gcp_sa.json"
        with open(path, "w", encoding="utf-8") as f: f.write(key_json)
        os.environ["GOOGLE_APPLICATION_CREDENTIALS"] = path
    except: pass

async def read_limited(upload: UploadFile, limit: int) -> bytes:
    chunks = []
    size = 0
    while True:
        chunk = await upload.read(1024 * 1024)
        if not chunk: break
        size += len(chunk)
        if size > limit: raise HTTPException(status_code=413, detail="Dosya √ßok b√ºy√ºk.")
        chunks.append(chunk)
    return b"".join(chunks)

# =======================================================
# 4) CORE ALGORƒ∞TMA: DETERMƒ∞Nƒ∞STƒ∞K ANALƒ∞Z (REGEX)
# =======================================================
def analyze_deterministic(text: str) -> List[Dict[str, Any]]:
    errors = []
    sentence_starts = get_sentence_starts(text)
    
    # 1. STANDART REGEX HATALARI (yada, her≈üey...)
    for rule_id, pattern in PATTERNS.items():
        for match in pattern.finditer(text):
            whole_word = match.group(0)
            
            # ƒ∞stisna Kontrol√º (MI Eki)
            if rule_id == "TDK_03_SORU_EKI":
                stem = match.group(1)
                suffix = match.group(2)
                if tr_lower(whole_word) in MI_SUFFIX_BLACKLIST:
                    continue 
                correct = f"{stem} {suffix}"
                explanation = "Soru eki 'mi/mƒ±' her zaman ayrƒ± yazƒ±lƒ±r."
            
            elif rule_id == "TDK_04_SEY_AYRI":
                stem = match.group(1)
                correct = f"{stem} ≈üey"
                explanation = "'≈ûey' s√∂zc√ºƒü√º her zaman ayrƒ± yazƒ±lƒ±r."
            elif rule_id == "TDK_06_YA_DA": 
                correct = "ya da"
                explanation = "'Ya da' baƒülacƒ± ayrƒ± yazƒ±lƒ±r."
            elif rule_id == "TDK_07_HER_SEY": 
                correct = "her ≈üey"
                explanation = "'Her ≈üey' ayrƒ± yazƒ±lƒ±r."
            elif rule_id == "TDK_44_BIRKAC": 
                correct = "birka√ß"
                explanation = "'Birka√ß' kelimesi biti≈üik yazƒ±lƒ±r."
            elif rule_id == "TDK_45_HICBIR": 
                correct = "hi√ßbir"
                explanation = "'Hi√ßbir' kelimesi biti≈üik yazƒ±lƒ±r."
            elif rule_id == "TDK_46_PEKCOK": 
                correct = "pek √ßok"
                explanation = "'Pek √ßok' ayrƒ± yazƒ±lƒ±r."
            elif rule_id == "TDK_41_HERKES": 
                correct = "herkes"
                explanation = "'Herkes' kelimesi 's' ile yazƒ±lƒ±r."
            elif rule_id == "TDK_42_YALNIZ": 
                correct = "yalnƒ±z"
                explanation = "Yalƒ±n k√∂k√ºnden gelir, 'yalnƒ±z' yazƒ±lƒ±r."
            elif rule_id == "TDK_43_YANLIS": 
                correct = "yanlƒ±≈ü"
                explanation = "Yanƒ±lmak k√∂k√ºnden gelir, 'yanlƒ±≈ü' yazƒ±lƒ±r."
            elif rule_id == "TDK_47_INSALLAH": 
                correct = "in≈üallah"
                explanation = "Doƒüru yazƒ±m 'in≈üallah' ≈üeklindedir."
            
            elif rule_id == "TDK_23_KESME_GENEL":
                stem = match.group(1)
                suffix = match.group(2)
                correct = f"{stem}{suffix}"
                explanation = "Cins isimlere gelen ekler kesme i≈üaretiyle ayrƒ±lmaz."

            errors.append({
                "wrong": whole_word,
                "correct": correct,
                "rule_id": rule_id,
                "span": {"start": match.start(), "end": match.end()},
                "type": "Yazƒ±m",
                "explanation": explanation,
                "confidence": 1.0,
                "source": "RULE_BASED"
            })

    # 2. √ñZEL ƒ∞Sƒ∞M SONEK ANALƒ∞Zƒ∞ (Ahmetin -> Ahmet'in)
    # Ayrƒ±ca burada "Okula -> Okul'a" gibi false-positive'leri yakalayƒ±p "Gereksiz B√ºy√ºk Harf"e √ßeviriyoruz.
    for match in PROPER_NOUN_SUFFIX_REGEX.finditer(text):
        whole_word = match.group(0)
        stem = match.group(1)
        suffix = match.group(2)
        start_idx = match.start()
        is_sentence_start = start_idx in sentence_starts
        
        # 1. Eƒüer kelimenin kendisi Whitelist'te ise (√ñrn: Samsun), b√∂lme! (Sams'un DEME)
        if tr_lower(whole_word) in PROPER_NOUNS_WHITELIST:
            continue

        # 2. Eƒüer kelimenin k√∂k√º Cins ƒ∞simse (√ñrn: Okul, Kitap, ≈ûehir) -> Bu √∂zel isim hatasƒ± deƒüil, b√ºy√ºk harf hatasƒ±dƒ±r.
        # "Okula" -> "Okul'a" deƒüil, "okula" olmalƒ±.
        if tr_lower(stem) in COMMON_NOUNS:
            errors.append({
                "wrong": whole_word,
                "correct": tr_lower(whole_word),
                "rule_id": "TDK_12_GEREKSIZ_BUYUK",
                "span": {"start": start_idx, "end": match.end()},
                "type": "B√ºy√ºk Harf",
                "explanation": "Cins isimler c√ºmle ortasƒ±nda k√º√ß√ºk harfle yazƒ±lƒ±r.",
                "confidence": 0.95,
                "source": "RULE_BASED"
            })
            continue

        # 3. Ger√ßekten √ñzel ƒ∞simse (Ahmetin, Samsuna) -> Kesme i≈üareti √∂ner.
        if (not is_sentence_start) or (tr_lower(stem) in PROPER_NOUNS_WHITELIST):
            errors.append({
                "wrong": whole_word,
                "correct": f"{stem}'{suffix}",
                "rule_id": "TDK_20_KESME_OZEL_AD",
                "span": {"start": start_idx, "end": match.end()},
                "type": "Noktalama",
                "explanation": "√ñzel isimlere gelen ekler kesme i≈üareti ile ayrƒ±lƒ±r.",
                "confidence": 0.95,
                "source": "RULE_BASED"
            })

    # 3. GEREKSƒ∞Z B√úY√úK HARF TARAMASI (Ek almamƒ±≈ü kelimeler i√ßin: √áok, ≈ûehir vb.)
    for match in CAPITALIZED_WORD_REGEX.finditer(text):
        whole_word = match.group(0)
        start_idx = match.start()
        
        # C√ºmle ba≈üƒ±ysa veya Whitelist'teyse (Ahmet, Samsun) dokunma.
        if start_idx in sentence_starts: continue
        if tr_lower(whole_word) in PROPER_NOUNS_WHITELIST: continue
        
        # Eƒüer bu kelime zaten yukarƒ±daki PROPER_NOUN_SUFFIX_REGEX ile yakalandƒ±ysa (Okula), tekrar ekleme.
        already_found = any(e['span']['start'] == start_idx for e in errors)
        if already_found: continue

        # Geriye kalanlar potansiyel hata: "√áok", "≈ûehir"
        # Eƒüer cins isim listesindeyse kesin hata diyebiliriz.
        if tr_lower(whole_word) in COMMON_NOUNS:
             errors.append({
                "wrong": whole_word,
                "correct": tr_lower(whole_word),
                "rule_id": "TDK_12_GEREKSIZ_BUYUK",
                "span": {"start": start_idx, "end": match.end()},
                "type": "B√ºy√ºk Harf",
                "explanation": "K√º√ß√ºk harfle ba≈ülamalƒ±.",
                "confidence": 0.90,
                "source": "RULE_BASED"
            })

    return errors

# =======================================================
# 5) ENDPOINTS
# =======================================================

@app.get("/check-class/{code}")
async def check_class_code(code: str):
    try:
        response = supabase.table("classrooms").select("name").eq("code", code.upper().strip()).execute()
        if response.data: return {"valid": True, "class_name": response.data[0]["name"]}
        return {"valid": False}
    except: return {"valid": False}

@app.post("/ocr")
async def ocr_image(file: UploadFile = File(...), classroom_code: str = Form(...)):
    try:
        await ensure_gcp_credentials()
        file_content = await read_limited(file, MAX_FILE_SIZE)
        
        filename = f"{uuid.uuid4()}.jpg"
        image_url = ""
        try:
            supabase.storage.from_("odevler").upload(filename, file_content, {"content-type": "image/jpeg"})
            image_url = supabase.storage.from_("odevler").get_public_url(filename)
        except: pass

        try: vision_client = vision.ImageAnnotatorClient()
        except: return {"status": "error", "message": "Vision API Hatasƒ±"}

        image = vision.Image(content=file_content)
        context = vision.ImageContext(language_hints=["tr"])
        response = vision_client.document_text_detection(image=image, image_context=context)
        if response.error.message: return {"status": "error", "message": response.error.message}

        CONFIDENCE_THRESHOLD = 0.40
        masked_parts, raw_parts = [], []
        PUNCTUATION = set(".,;:!?\"'‚Äô`()-‚Äì‚Äî‚Ä¶")

        def append_break(break_type_val: int):
            if not break_type_val: return
            if break_type_val in (1, 2):
                masked_parts.append(" "); raw_parts.append(" ")
            elif break_type_val in (3, 5):
                masked_parts.append("\n"); raw_parts.append("\n")

        for page in response.full_text_annotation.pages:
            for block in page.blocks:
                for paragraph in block.paragraphs:
                    for word in paragraph.words:
                        for symbol in word.symbols:
                            ch = symbol.text or ""
                            conf = getattr(symbol, "confidence", 1.0)
                            raw_parts.append(ch)
                            if ch in PUNCTUATION: masked_parts.append(ch)
                            elif ch.isalpha(): masked_parts.append("‚ç∞" if conf < CONFIDENCE_THRESHOLD else ch)
                            else: masked_parts.append(ch)
                            prop = getattr(symbol, "property", None)
                            db = getattr(prop, "detected_break", None) if prop else None
                            if db: append_break(int(getattr(db, "type_", getattr(db, "type", 0))))

        raw_text = unicodedata.normalize("NFC", "".join(raw_parts).strip())
        masked_text = unicodedata.normalize("NFC", "".join(masked_parts).strip())

        # ≈û√ºpheli OCR D√ºzeltmeleri
        def force_suspect(t: str) -> str:
            t = re.sub(r"\b[gG]ok\b", lambda m: "‚ç∞"+m.group(0)[1:], t)
            return re.sub(r"\b[gG]ay\b", lambda m: "‚ç∞"+m.group(0)[1:], t)
        
        masked_text = force_suspect(masked_text)

        return {"status": "success", "ocr_text": masked_text, "raw_ocr_text": raw_text, "image_url": image_url}
    except Exception as e: return {"status": "error", "message": str(e)}

@app.post("/analyze")
async def analyze_submission(data: AnalyzeRequest):
    if not data.ocr_text or not data.ocr_text.strip():
        raise HTTPException(status_code=400, detail="Metin bo≈ü.")
    if "‚ç∞" in data.ocr_text:
        raise HTTPException(status_code=400, detail="√ñnce ‚ç∞ i≈üaretlerini d√ºzeltin.")

    full_text = normalize_text(data.ocr_text)
    print(f"üß† Hƒ∞BRƒ∞T ANALƒ∞Z BA≈ûLIYOR: {data.student_name} ({data.level})")

    # A≈ûAMA 1: Deterministik
    rule_errors = analyze_deterministic(full_text)
    
    # A≈ûAMA 2: LLM
    prompt = f"""
    G√ñREV: A≈üaƒüƒ±daki √∂ƒürenci metnini analiz et.
    
    √ñNEMLƒ∞:
    1. Zaten regex ile bulunan (-de/-da, -ki, mi/mƒ±, √∂zel isimler) hatalarƒ± yoksay.
    2. Sadece regex'in bulamadƒ±ƒüƒ± (glince -> gelince) gibi bozukluklarƒ± bul.
    3. ASLA metni deƒüi≈ütirme, sadece JSON ver.

    METƒ∞N:
    {full_text}

    √áIKTI FORMATI (JSON):
    {{ "additional_errors": [ {{ "wrong": "...", "correct": "...", "explanation": "..." }} ] }}
    """
    
    llm_errors = []
    # CEFR
    prompt_rubric = f"""
    ROL: √ñƒüretmen ({data.level}).
    METƒ∞N: \"\"\"{full_text}\"\"\"
    PUANLA (TOPLAM 100):
    Uzunluk(16), Noktalama(14), Dil Bilgisi(16), S√∂z Dizimi(20), Kelime(14), ƒ∞√ßerik(20).
    √áIKTI: {{ "rubric": {{ "uzunluk": 0, "noktalama": 0, "dil_bilgisi": 0, "soz_dizimi": 0, "kelime": 0, "icerik": 0 }}, "teacher_note": "..." }}
    """

    final_result = None

    for model_name in MODELS_TO_TRY:
        try:
            # 1. Hata Tespiti
            resp_err = await asyncio.to_thread(
                client.models.generate_content,
                model=model_name,
                contents=prompt,
                config=types.GenerateContentConfig(response_mime_type="application/json")
            )
            llm_json = safe_json(getattr(resp_err, "text", "") or "")
            raw_llm_errors = llm_json.get("additional_errors", [])

            # 2. Puanlama
            resp_rubric = await asyncio.to_thread(
                client.models.generate_content,
                model=model_name,
                contents=prompt_rubric,
                config=types.GenerateContentConfig(response_mime_type="application/json", temperature=0.1)
            )
            rubric_json = safe_json(getattr(resp_rubric, "text", "") or "")

            # LLM E≈üle≈ütirme
            for item in raw_llm_errors:
                wrong_word = item.get("wrong", "")
                if not wrong_word: continue
                match = re.search(re.escape(wrong_word), full_text)
                if match:
                    is_overlap = any(
                        (match.start() < e["span"]["end"] and match.end() > e["span"]["start"])
                        for e in rule_errors
                    )
                    if not is_overlap:
                        llm_errors.append({
                            "wrong": wrong_word,
                            "correct": item.get("correct"),
                            "rule_id": "LLM_SEMANTIC",
                            "span": {"start": match.start(), "end": match.end()},
                            "type": "Kelime Hatasƒ±",
                            "explanation": item.get("explanation"),
                            "confidence": 0.85,
                            "source": "LLM"
                        })

            all_errors = rule_errors + llm_errors
            all_errors.sort(key=lambda x: x["span"]["start"])

            rb = rubric_json.get("rubric", {})
            rubric = {
                "uzunluk": to_int(rb.get("uzunluk"), 10),
                "noktalama": to_int(rb.get("noktalama"), 10),
                "dil_bilgisi": to_int(rb.get("dil_bilgisi"), 10),
                "soz_dizimi": to_int(rb.get("soz_dizimi"), 15),
                "kelime": to_int(rb.get("kelime"), 10),
                "icerik": to_int(rb.get("icerik"), 15),
            }
            total_score = sum(rubric.values())

            final_result = {
                "score_total": total_score,
                "rubric": rubric,
                "errors": all_errors,
                "errors_ocr": [], 
                "teacher_note": rubric_json.get("teacher_note", "Analiz tamamlandƒ±."),
                "ai_insight": "Hibrit analiz (Kural + YZ) tamamlandƒ±."
            }
            break

        except Exception as e:
            print(f"LLM Hata ({model_name}): {e}")
            continue

    if not final_result:
        raise HTTPException(status_code=500, detail="Analiz ba≈üarƒ±sƒ±z oldu.")

    try:
        supabase.table("submissions").insert({
            "student_name": data.student_name,
            "student_surname": data.student_surname,
            "classroom_code": data.classroom_code,
            "image_url": data.image_url,
            "ocr_text": full_text,
            "level": data.level,
            "country": data.country,
            "native_language": data.native_language,
            "analysis_json": final_result,
            "score_total": final_result["score_total"]
        }).execute()
    except Exception as e:
        print(f"DB Kayƒ±t Hatasƒ±: {e}")

    return {"status": "success", "data": final_result}

@app.post("/student-history")
async def get_student_history(student_name: str = Form(...), student_surname: str = Form(...), classroom_code: str = Form(...)):
    try:
        res = supabase.table("submissions").select("*")\
            .ilike("student_name", student_name.strip())\
            .ilike("student_surname", student_surname.strip())\
            .eq("classroom_code", classroom_code.strip())\
            .order("created_at", desc=True).execute()
        return {"status": "success", "data": res.data}
    except Exception as e: return {"status": "error", "message": str(e)}

@app.post("/update-score")
async def update_score(data: UpdateScoreRequest):
    try:
        res = supabase.table("submissions").select("analysis_json").eq("id", data.submission_id).execute()
        if not res.data: raise HTTPException(status_code=404, detail="Kayƒ±t yok")
        curr = res.data[0]["analysis_json"]
        if "rubric" not in curr: curr["rubric"] = {}
        curr["rubric"].update(data.new_rubric)
        supabase.table("submissions").update({ "score_total": data.new_total, "analysis_json": curr }).eq("id", data.submission_id).execute()
        return {"status": "success", "message": "G√ºncellendi"}
    except Exception as e: raise HTTPException(status_code=500, detail=str(e))